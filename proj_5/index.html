<!DOCTYPE html>
<html lang="en" class="theme-light" data-theme="light">
  <head>
    <meta charset="UTF-8" />
    <title>Project 5: Fun With Diffusion Models!</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="../style.css" />
    <link rel="stylesheet" href="proj_5.css" />
    <link
      id="hljs-theme"
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/styles/github.min.css"
    />
    <script
      src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/highlight.min.js"
      defer
    ></script>
    <script
      src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/languages/python.min.js"
      defer
    ></script>
    <script defer>
      window.addEventListener("DOMContentLoaded", () => {
        if (window.hljs) hljs.highlightAll();
      });
    </script>
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
        },
        options: {
          skipHtmlTags: [
            "script",
            "noscript",
            "style",
            "textarea",
            "pre",
            "code",
            "figure",
          ],
        },
      };
    </script>
    <script
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>
  <body class="theme-light" data-theme="light">
    <nav class="top-navbar">
      <div class="navbar-left">
        <a class="breadcrumb" href="../index.html">CS280A</a>
        <span class="breadcrumb-sep"></span>
        <span class="breadcrumb" aria-current="page">Project 5</span>
      </div>
      <div class="navbar-right">
        <button
          id="sidebar-toggle"
          class="top-icon-btn"
          aria-label="Toggle sidebar"
          title="Toggle sidebar"
        >
          ‚ò∞
        </button>
        <button
          id="theme-toggle"
          class="top-icon-btn"
          aria-label="Toggle color theme"
          title="Toggle color theme"
        >
          üåì
        </button>
      </div>
    </nav>

    <aside id="sidebar" class="sidebar collapsed">
      <nav>
        <ul>
          <li>
            <a href="#part-A">Part A: The Power of Diffusion Models!</a>
            <ul>
              <li><a href="#part-A-0">A.0: Setup</a></li>
              <li><a href="#part-A-1">A.1: Sampling Loops</a></li>
              <li><a href="#part-A-1-1">A.1.1: Forward Process</a></li>
              <li><a href="#part-A-1-2">A.1.2: Classical Denoising</a></li>
              <li><a href="#part-A-1-3">A.1.3: One-Step Denoising</a></li>
              <li><a href="#part-A-1-4">A.1.4: Iterative Denoising</a></li>
              <li><a href="#part-A-1-5">A.1.5: Diffusion Model Sampling</a></li>
              <li><a href="#part-A-1-6">A.1.6: Classifier-Free Guidance</a></li>
              <li>
                <a href="#part-A-1-7">A.1.7: Image-to-image Translation</a>
              </li>
              <li>
                <a href="#part-A-1-7-1"
                  >A.1.7.1: Editing Hand-Drawn and Web Images</a
                >
              </li>
              <li><a href="#part-A-1-7-2">A.1.7.2: Inpainting</a></li>
              <li>
                <a href="#part-A-1-7-3"
                  >A.1.7.3: Text-Conditional Image-to-image Translation</a
                >
              </li>

              <li><a href="#part-A-1-8">A.1.8: Visual Anagrams</a></li>
              <li><a href="#part-A-1-9">A.1.9: Hybrid Images</a></li>
              <li>
                <a href="#part-A-bw1">B &amp; W: More visual anagrams!</a>
                <a href="#part-A-bw2">B &amp; W: Design a course logo!</a>
              </li>
            </ul>

            <a href="#part-B">Part B: Flow Matching from Scratch!</a>
            <ul>
              <li>
                <a href="#part-B-1"
                  >B.1: Training a Single-Step Denoising UNet</a
                >
              </li>
              <li><a href="#part-B-1-1">B.1.1: Implementing the UNet</a></li>
              <li>
                <a href="#part-B-1-2"
                  >B.1.2: Using the UNet to Train a Denoiser</a
                >
              </li>
              <li><a href="#part-B-1-2-1">B.1.2.1: Training</a></li>
              <li>
                <a href="#part-B-1-2-2">B.1.2.2: Out-of-Distribution Testing</a>
              </li>
              <li><a href="#part-B-1-2-3">B.1.2.3: Denoising Pure Noise</a></li>
              <li>
                <a href="#part-B-2">B.2: Training a Flow Matching Model</a>
              </li>
              <li>
                <a href="#part-B-2-1"
                  >B.2.1: Adding Time Conditioning to UNet</a
                >
              </li>
              <li><a href="#part-B-2-2">B.2.2: Training the UNet</a></li>
              <li><a href="#part-B-2-3">B.2.3: Sampling from the UNet</a></li>
              <li>
                <a href="#part-B-2-4"
                  >B.2.4: Adding Class-Conditioning to UNet</a
                >
              </li>
              <li><a href="#part-B-2-5">B.2.5: Training the UNet</a></li>
              <li><a href="#part-B-2-6">B.2.6: Sampling from the UNet</a></li>
              <li>
                <a href="#part-B-bw">B &amp; W: Better Time-Conditioned UNet</a>
              </li>
            </ul>
          </li>
        </ul>
      </nav>
    </aside>
    <div class="overlay" id="sidebar-overlay" aria-hidden="true"></div>

    <main class="with-sidebar">
      <div class="main-center">
        <header>
          <h1>Fun With Diffusion Models!</h1>
        </header>

        <div class="figure-center">
          <img
            src="media/part-a/bw_negative_illusion_seed68.png"
            class="intro-img-pair"
          />
        </div>

        <section id="introduction">
          <h2><u>Introduction</u></h2>
          <p>
            In this project, we explore diffusion models for generative image
            tasks. Part A focuses on understanding and experimenting with
            diffusion sampling, including forward and reverse processes,
            classical and learned denoising, classifier-free guidance,
            image-to-image translation, inpainting, and multi-view optical
            illusions. These techniques allow us to understand how modern
            diffusion models such as DeepFloyd IF generate and edit images.
            Then, in part B, we will build and train our own
            <a href="https://arxiv.org/abs/2210.02747">flow matching</a> model
            on MNIST.
          </p>
          <div class="discussion-card">
            <h3>Highlights</h3>
            <ul>
              <li>Diffusion models for image generation</li>
              <li>Forward & Reverse Sampling Loops</li>
              <li>Classical, One-Step, and Iterative Denoising</li>
              <li>Classifier-Free Guidance</li>
              <li>Image-to-image Translation (SDEdit)</li>
              <li>Inpainting and Shape Completion</li>
              <li>Visual Illusions & Hybrid Images</li>
              <li>Flow Matching Models</li>
            </ul>
          </div>
        </section>
        <hr />

        <section id="part-A">
          <h2><u>Part A: The Power of Diffusion Models!</u></h2>
          <p>
            In part A, we will play with pretrained diffusion models to perform
            various image generation tasks and sampling techniques. We will use
            the
            <a
              href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if"
              >DeepFloyd IF</a
            >
            diffusion model here. All implementations will follow the
            instructions in the
            <a
              href="https://colab.research.google.com/drive/19mp-ssAv3CQuVvFsUu2VvWEwnqLds9gx?usp=sharing"
              >provided notebook</a
            >
            and
            <a href="https://cal-cs180.github.io/fa25/hw/proj5/parta.html"
              >project page</a
            >.
          </p>
        </section>

        <section id="part-A-0">
          <h3><u>A.0: Setup & Play with DeepFloyd</u></h3>
          <p>
            Once we have access to DeepFloyd through
            <a href="https://huggingface.co">Hugging Face</a>, we can start
            playing with this model by sampling images conditioned on our
            self-crafted text prompts. The text prompts were encoded via a
            pretrained text encoder, in this case, the encoder of a
            <a href="https://arxiv.org/abs/1910.10683">T5</a> model was used,
            which embeds each plain text prompt into a $[1, 77, 4096]^{\top}$
            vector, where $77$ is the maximum token length (sentences longer
            than this will be truncated) and $4096$ is the embedding dimension.
            The text embeddings are then fed into the diffusion model to guide
            the image generation process.
          </p>
          <h4><u>Experiment</u>: Effect of <code>num_inference_steps</code></h4>
          <p>
            Let's generate some images using DeepFloyd conditioned on some of
            our text prompts. Recall that the
            <code>num_inference_steps</code> will affect the quality and
            diversity of generated images, so here we will explore different
            values to observe their impact.
          </p>
          <p>
            <em
              >All Part A experiments were generated using seed=100 (as shown in
              figures).</em
            >
          </p>

          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a0_seed100_steps10.png"
                data-full="media/part-a/a0_seed100_steps10.png"
                alt="seed=100, num_inference_steps=10"
              />
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a0_seed100_steps100.png"
                data-full="media/part-a/a0_seed100_steps100.png"
                alt="seed=100, num_inference_steps=100"
              />
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a0_seed100_steps999.png"
                data-full="media/part-a/a0_seed100_steps999.png"
                alt="seed=100, num_inference_steps=999"
              />
            </div>
          </figure>
          <p>
            From the results above, we can see that with a low number of
            <code>num_inference_steps</code>, e.g., 10, the generated image will
            be of low quality and may not fully ground on the text prompt. For
            instance, the The panda's body is in an unnatural posture, and we
            got $2$ stormtroopers instead of $1$ as specified in the prompt. As
            we increase the number of inference steps, say, to $100$, the image
            quality improves significantly and the details align better with the
            prompt. However, this improvement in quality will have certain
            marginal benefits, which can be proved by the fact that further
            increasing the steps to $999$ does not yield a substantial
            improvement compared to $100$ steps.
          </p>

          <h4><u>Experiment</u>: Effect of Prompts</h4>
          <p>
            Beyound <code>num_inference_steps</code>, text prompts for the same
            sense of image with varying details will also affect the generated
            results, so let's try a few different prompts for our panda. One can
            observe that once the <code>num_inference_steps</code> is set to a
            reasonably large value (e.g., $500$), the generated images will be
            well aligned with the prompts, demonstrating the model's strong
            capability in grounding textual descriptions to visual content.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a0_seed100_steps500_diff_prompts.png"
                data-full="media/part-a/a0_seed100_steps500_diff_prompts.png"
                alt="seed=100, num_inference_steps=500, different prompts"
              />
            </div>
          </figure>
        </section>

        <section id="part-A-1">
          <h3><u>A.1: Sampling Loops</u></h3>
          <p>
            In this part, we will implement our own "sampling loops" that use
            the pretrained DeepFloyd denoisers, which should produce
            high-quality images such as the ones generated before. We will then
            modify these sampling loops to solve different tasks, such as
            inpainting or producing optical illusions.
          </p>
          <p>
            Starting with a clean image $x_0$, we can iteratively add noise to
            an image, obtaining progressively noisier images $x_1, x_2, \ldots,
            x_t$, until we are left with pure noise at timestep $t=T$, hence,
            $x_0$ is our clean image, and for larger $t$ more noise is in the
            image.
          </p>
          <p>
            A diffusion model reverses this process by denoising the image
            through predicting the noise component given a noisy $x_t$ and the
            timestep $t$. Each iteration, we can either remove all noise in one
            step, or remove a small amount of noise, obtaining a slightly
            cleaner image $x_{t-1}$, and repeat this process until we reach
            $x_0$. This means we can also start from pure noise $x_T$ and
            iteratively denoise it to obtain a clean image $x_0$.
          </p>
          <p>
            For the DeepFloyd models, $T=1000$, and the exact amount of noise
            added at each step is dictated by noise coefficients
            $\bar{\alpha}_t$, which were chosen by the people who trained the
            model.
          </p>
        </section>

        <section id="part-A-1-1">
          <h3><u>A.1.1: Forward Process</u></h3>
          <p>
            As the key part of diffusion, here we first implement the forward
            process, i.e., take a clean image and add noise to it, defined by:
            $$ q(x_t \mid x_0)=\mathcal{N}\left(x_t ; \sqrt{\bar{\alpha}_t} x_0,
            (1-\bar{\alpha}_t) \mathbf{I}\right),\tag{A.1}$$ which is equivalent
            to computing $$ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 -
            \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0,
            1).\tag{A.2}$$ Therefore, for a given clean image $x_0$ and timestep
            $t$, the noisy image $x_t$ can be obtained by sampling $\epsilon$
            from a standard normal distribution and adding it to the scaled
            clean image.
          </p>
          <h4>Implementation & Results</h4>
          <p>
            The code snippet below shows the implementation of the forward
            process function, and the
            <a
              href="https://cal-cs180.github.io/fa25/hw/proj5/assets/campanile.jpg"
              >Campanile</a
            >
            at noise levels of <code>[250, 500, 750]</code> through the
            implemented function are also visualized as follows.
          </p>
          <figure class="code-card">
            <p>
              <em>Code Snippet:</em>
              <code>noisy_im = forward(im, t)</code> function
            </p>
            <pre><code class="language-python">def forward(im, t):
  """
  Args:
    im : torch tensor of size (1, 3, 64, 64) representing the clean image
    t : integer timestep

  Returns:
    im_noisy : torch tensor of size (1, 3, 64, 64) representing the noisy image at timestep t
  """
  with torch.no_grad():
    # ===== your code here! ====
    alpha_bar = alphas_cumprod[t].to(im.device).type(im.dtype)
    w_x0 = torch.sqrt(alpha_bar)
    w_epsilon = torch.sqrt(1 - alpha_bar)
    
    epsilon = torch.randn_like(im)
    im_noisy = w_x0 * im + w_epsilon * epsilon
    # ===== end of code ====
  return im_noisy</code></pre>
          </figure>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-1_seed100_levels.png"
                data-full="media/part-a/a1-1_seed100_levels.png"
                alt="Campanile at t=[0,250,500,750]"
              />
            </div>
            <figcaption>
              Campanile at <code>t = [0,250,500,750]</code> (seed = 100)
            </figcaption>
          </figure>
        </section>

        <section id="part-A-1-2">
          <h3><u>A.1.2: Classical Denoising</u></h3>
          <p>
            Before denoising with the learned diffusion model, let's first see
            how classical methods perform. Here, we use
            <em>Gaussian blur filtering</em> to try to remove the noise in the
            noisy images that we obtained before. The results images before and
            after denoising are shown below. In my implementation, I used
            <code>sigma = 2</code> and
            <code>kernel_size = 6 * sigma + 1</code> for the Gaussian filter,
            i.e., covering over $99$% mass of the Gaussian distribution.
            Apparently, Gaussian blur is ineffective at removing this level of
            Gaussian additive noise, especially because the noisy image has lost
            high-frequency structure. Blurring only smooths the image, but
            cannot reconstruct lost details.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-2_seed100_gaussian_denoising.png"
                data-full="media/part-a/a1-2_seed100_gaussian_denoising.png"
                alt="Classical Denoising Results"
              />
            </div>
            <figcaption>
              Classical Denoising Results on Campanile at
              <code>t = [250,500,750]</code> (seed = 100)
            </figcaption>
          </figure>
        </section>

        <section id="part-A-1-3">
          <h3><u>A.1.3: One-Step Denoising</u></h3>
          <p>
            Now, let's use the pretrained diffusion model to perform the same
            denoising task in one step. Since the model is pretrained on a
            <em>very</em> large dataset of $(x_0, x_t)$, we can use it to
            recover the Gaussian noise conditioned on a certain timestep $t$ and
            remove it.
          </p>
          <p>
            For matching the input format of the model, since it was trained
            with text conditioning, we provide the embedding of the prompt "a
            high quality photo" as a neutral conditioning.
          </p>
          <h4>Implementation & Results</h4>
          <p>
            Again, the same noisy images at timesteps
            <code>t = [250, 500, 750]</code> are denoised using the one-step
            denoising function we implemented. The results are shown below,
            which demonstrate a significant improvement over classical
            denoising. The pretrained diffusion model effectively recovers much
            of the lost detail and structure in the images. However, with the
            increase of noise level, some artifacts start to appear in this
            one-step approach, indicating that a single denoising step may not
            be sufficient for very noisy images.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-3_seed100_one_step_denoising.png"
                data-full="media/part-a/a1-3_seed100_one_step_denoising.png"
                alt="One-Step Denoising Results"
              />
            </div>
            <p>
              One-Step Denoising Results on Campanile at
              <code>t = [250,500,750]</code> (seed = 100)
            </p>
          </figure>
        </section>

        <section id="part-A-1-4">
          <h3><u>A.1.4: Iterative Denoising</u></h3>
          <p>
            In part A.1.3, we saw that one-step denoising by the diffusion model
            does a much better job than classical approaches such as Gaussian
            blur. However, we also noticed that it does get worse as we add more
            noise, which makes sense, as the problem is much harder with more
            noise!
          </p>
          <p>
            To address this, recall that diffusion models are designed to
            denoise iteratively, so here we implement an iterative denoising
            loop that gradually removes noise over multiple steps.
          </p>
          <p>
            In theory, we could start with $x_{1000}$ at timestep $T=1000$,
            denoise one step each iteration, and carry on until we reach $x_0$.
            However, such a long chain will hinder efficiency, so instead, we
            can actually speed things up by skipping steps, whose theoretical
            foundation can be found in
            <a href="https://yang-song.net/blog/2021/score/">this article</a>.
          </p>
          <p>
            To skip steps, we can define a new list of timesteps called
            <code>strided_timesteps</code>, which is a subset of the original
            timesteps, hence, <code>strided_timesteps[0]</code> corresponds to
            the largest $t$ (and thus the noisiest image), and
            <code>strided_timesteps[-1]</code> corresponds to $t=0$ (the clean
            image). In my implementation, the stride was set to $30$.
          </p>
          <p>
            On the <code>i</code>th step, i.e.,
            $t=$<code>strided_timesteps[i]</code>, we denoise the image to
            obtain a less noisy image $t'=$<code>strided_timesteps[i+1]</code>,
            the calculation follows: $$ x_{t'} =
            \frac{\sqrt{\bar\alpha_{t'}}\beta_t}{1 - \bar\alpha_t} x_0 +
            \frac{\sqrt{\alpha_t}(1 - \bar\alpha_{t'})}{1 - \bar\alpha_t} x_t +
            v_\sigma,\tag{A.3} $$ where:
          </p>
          <ul>
            <li>$x_t$ is your image at timestep $t$</li>
            <li>
              $x_{t'}$ is your noisy image at timestep $t'$ where $t' < t$ (less
              noisy)
            </li>
            <li>
              $\bar\alpha_t$ is defined by `alphas_cumprod`, as explained above.
            </li>
            <li>$\alpha_t = \bar\alpha_t / \bar\alpha_{t'}$</li>
            <li>$\beta_t = 1 - \alpha_t$</li>
            <li>
              $x_0$ is our current estimate of the clean image (computed using
              the one-step denoising formula from the previous section).
            </li>
          </ul>
          <p>
            The $v_{\sigma}$ is the additional noise term predicted by
            DeepFloyd, added via the provided
            <code>add_variance</code> function.
          </p>

          <h4>Implementation & Results</h4>
          <p>
            Here we first create the <code>strided_timesteps</code> by the
            following snippet:
          </p>
          <figure class="code-card">
            <p>
              <em>Code Snippet:</em>
              create <code>strided_timesteps</code>
            </p>
            <pre><code class="language-python">strided_timesteps = list(range(990, -1, -30))</code></pre>
          </figure>
          <p></p>
          <p>
            Then, starting with the given function framework in the notebook, we
            can implement the iterative denoising loop, as shown below.
          </p>
          <figure class="code-card">
            <p>
              <em>Code Snippet:</em>
              complete <code>iterative_denoise</code>
            </p>
            <pre><code class="language-python">def iterative_denoise(im_noisy, i_start, prompt_embeds, timesteps, display=True):
  # ==== my modifications ====
  image = im_noisy.to(device).half()
  prompt_embeds = prompt_embeds.half().to(device)
  intermediate_img_t = []
  # ==== end of modifications ====

  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      # Get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # get `alpha_cumprod` and `alpha_cumprod_prev` for timestep t from `alphas_cumprod`
      # compute `alpha`
      # compute `beta`
      # ===== your code here! =====
      alpha_cumprod = alphas_cumprod[t].to(image.device).type(image.dtype)
      alpha_cumprod_prev = alphas_cumprod[prev_t].to(image.device).type(image.dtype)
      alpha = alpha_cumprod / alpha_cumprod_prev
      beta = 1 - alpha
      # ==== end of code ====

      # Get noise estimate
      model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt_embeds,
          return_dict=False
      )[0]

      # Split estimate into noise and variance estimate
      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)

      # compute `pred_prev_image` (x_{t'}), the DDPM estimate for the image at the
      # next timestep, which is slightly less noisy. Use the equation 3.
      # This is the core of DDPM
      # ===== your code here! =====
      # compute `x_0` using one-step denoising
      x_t = image
      w_xt = 1 / torch.sqrt(alpha_cumprod)
      w_sigma = torch.sqrt((1 - alpha_cumprod) / (alpha_cumprod))
      x_0 = w_xt * x_t - w_sigma * noise_est
      
      # compute `pred_prev_image` (x_{t'})
      w_x0 = (torch.sqrt(alpha_cumprod_prev) * beta) / (1 - alpha_cumprod)
      w_xt = (torch.sqrt(alpha) * (1 - alpha_cumprod_prev)) / (1 - alpha_cumprod)
      xt_prev = w_x0 * x_0 + w_xt * x_t
      pred_prev_image = add_variance(predicted_variance, t, xt_prev)
      
      # store every 5th loop of denoising
      if i % 5 == 0 and display:
        intermediate_img_t.append((pred_prev_image,t))
      # ==== end of code ====

      image = pred_prev_image

    clean = image.cpu().detach().numpy()

  # ==== my modifications ====
  if display:
    return clean, intermediate_img_t
  else:
    return clean
  # ==== end of modifications ====</code></pre>
          </figure>
          <p>
            With <code>i_start = 10</code>, the noisy Campanile every $5$th loop
            of denoising is shown below, where we can see that they becomes
            gradually cleaner.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-4_seed100_iterative_denoising_intermediates.png"
                data-full="media/part-a/a1-4_seed100_iterative_denoising_intermediates.png"
                alt="Iterative Denoising Intermediate Steps"
              />
            </div>
            <figcaption>
              Iterative Denoising Intermediate Steps on Campanile with
              <code>i_start = 10</code> (seed = 100)
            </figcaption>
          </figure>
          <p>
            As a comparison, the original image and final denoised images after
            the aforementioned approaches are shown below, and we can see that
            the iterative denoising produces the best results, effectively
            recovering fine details and structure from the noisy input.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-4_seed100_denoising_comparison.png"
                data-full="media/part-a/a1-4_seed100_denoising_comparison.png"
                alt="Final Denoising Comparison"
              />
            </div>
            <figcaption>
              Final Denoising Comparison on Campanile at
              <code>i_start = 10</code> (seed = 100)
            </figcaption>
          </figure>
        </section>

        <section id="part-A-1-5">
          <h3><u>A.1.5: Diffusion Model Sampling</u></h3>
          <p>
            Diffusion models can not only denoise images, but also able to
            generate images from scratch by starting from pure noise, i.e.,
            setting <code>i_start = 0</code> and passing random noise as our
            <code>im_noisy</code> to our
            <code>iterative_denoise</code> function. Recall that DeepFloyd
            requires text conditioning as input, here we use a neutral prompt "a
            high quality photo" again.
          </p>
          <h4>Implementation & Results</h4>
          <p>
            To sample images from scratch, we can generate random noise of shape
            $(1,3,64,64)$ via
            <code>im_noisy = torch.randn(1, 3, 64, 64).half().to(device)</code>,
            and pass it to our <code>iterative_denoise</code> function with
            <code>i_start = 0</code> and embedding of the neutral prompt. For
            reproducibility purposes, I sample images with
            <code>seeds = [0, 1, 3, 4, 5]</code>, each of which produces a
            distinct image as shown below.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-5_samples.png"
                data-full="media/part-a/a1-5_samples.png"
                alt="Diffusion Model Sampling Results"
              />
            </div>
            <figcaption>
              Diffusion Model Sampling Results with
              <code>seeds = [0,1,3,4,5]</code>
            </figcaption>
          </figure>
          <p>
            Obviously, the sampled images are of high quality and look like real
            photos, hence, they lie on the natural image manifold somehow.
            However, some of them still look a bit strange and unnatural. We
            will fix this issue in the next part with CFG!
          </p>
        </section>

        <section id="part-A-1-6">
          <h3><u>A.1.6: Classifier-Free Guidance</u></h3>
          <p>
            As we noticed in the previous part that some of the generated images
            are not very good and even completely non-sensical, here we improve
            the quality (at the expense of image diversity) with a technique
            called
            <a href="https://arxiv.org/abs/2207.12598"
              >Classifier-Free Guidance (CFG)</a
            >, where we compute both a conditional ($\epsilon_c$) and an
            unconditional ($\epsilon_u$) noise estimate through: $$ \epsilon =
            \epsilon_u + \gamma(\epsilon_c - \epsilon_u),\tag{A.4} $$ where
            $\gamma$ controls the strength of CFG. From the equation, we can see
            that $\gamma=0$ corresponds to an unconditional noise estimate, and
            $\gamma=1$ corresponds to a conditional noise estimate. By setting
            $\gamma > 1$, we can push the noise estimate further towards the
            conditional direction, which encourages the model to generate images
            that better align with the text prompt, hence improving quality.
            More details can be found in
            <a href="https://sander.ai/2022/05/26/guidance.html"
              >this blog post</a
            >.
          </p>
          <p>
            In practice, the "unconditional" embedding corresponds to an empty
            prompt, i.e., "". Then, our previous "a high quality photo" prompt
            becomes a condition, even though it is still weak and neutral. We
            will use CFG with $\gamma=7$ all throughout all Part A experiments
            starting from here.
          </p>
          <h4>Implementation & Results</h4>
          <p>
            Implementation of CFG is straightforward, we just need to modify our
            <code>iterative_denoise</code> function to compute both conditional
            and unconditional noise estimates, and combine them via equation
            (4). The modified code snippet is shown below.
          </p>
          <figure class="code-card">
            <p>
              <em>Code Snippet:</em>
              implement CFG in <code>iterative_denoise</code>
            </p>
            <pre><code class="language-python">def iterative_denoise_cfg(im_noisy, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  # ==== my modifications ====
  image = im_noisy.half().to(device)
  prompt_embeds = prompt_embeds.half().to(device)
  uncond_prompt_embeds = uncond_prompt_embeds.half().to(device)
  # ==== end of modifications ====

  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      # Get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # Get `alpha_cumprod`, `alpha_cumprod_prev`, `alpha`, `beta`
      # ===== your code here! =====
      alpha_cumprod = alphas_cumprod[t].to(image.device).type(image.dtype)
      alpha_cumprod_prev = alphas_cumprod[prev_t].to(image.device).type(image.dtype)
      alpha = alpha_cumprod / alpha_cumprod_prev
      beta = 1 - alpha
      # ==== end of code ====

      # Get cond noise estimate
      model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt_embeds,
          return_dict=False
      )[0]

      # Get uncond noise estimate
      uncond_model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds,
          return_dict=False
      )[0]

      # Split estimate into noise and variance estimate
      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)
      uncond_noise_est, _ = torch.split(uncond_model_output, image.shape[1], dim=1)

      # Compute the CFG noise estimate based on equation 4
      # ===== your code here! =====
      epsilon_u = uncond_noise_est
      epsilon_c = noise_est
      epsilon = epsilon_u + scale * (epsilon_c - epsilon_u)
      # ==== end of code ====


      # Get `pred_prev_image`, the next less noisy image.
      # ===== your code here! =====
      # compute `x_0` using one-step denoising
      x_t = image
      w_xt = 1 / torch.sqrt(alpha_cumprod)
      w_sigma = torch.sqrt((1 - alpha_cumprod) / (alpha_cumprod))
      x_0 = w_xt * x_t - w_sigma * epsilon # use cfg epsilon here
      
      # compute `pred_prev_image` (x_{t'})
      w_x0 = (torch.sqrt(alpha_cumprod_prev) * beta) / (1 - alpha_cumprod)
      w_xt = (torch.sqrt(alpha) * (1 - alpha_cumprod_prev)) / (1 - alpha_cumprod)
      xt_prev = w_x0 * x_0 + w_xt * x_t
      pred_prev_image = add_variance(predicted_variance, t, xt_prev)
      # ==== end of code ====

      image = pred_prev_image

    clean = image.cpu().detach().numpy()

    return clean</code></pre>
          </figure>
          <p>
            With CFG, let's again sample some images from scratch and see how it
            improves the quality. Here I picked
            <code>seeds = [0, 2, 3, 4, 5]</code> and set <code>scale = 7</code>.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-6_cfg_samples.png"
                data-full="media/part-a/a1-6_cfg_samples.png"
                alt="CFG Sampling Results"
              />
            </div>
            <figcaption>
              CFG Sampling Results with
              <code>seeds = [0,2,3,4,5]</code>, &nbsp;
              <code>scale = 7</code>
            </figcaption>
          </figure>
          <p>
            The results above look much better than those without CFG, all of
            which now look like real photos and nothing non-sensical!
          </p>
        </section>

        <section id="part-A-1-7">
          <h3><u>A.1.7: Image-to-image Translation</u></h3>
          <p>
            The diffusion model can also be used to edit images, as we already
            saw in part 1.4 when we add noise to an image and then denoise it.
            This works because the diffusion model has to "hallucinate" new
            things to fill in the missing details in the noised image, hence,
            the model has to be "creative" and force the noisy image back onto
            the manifold of natural images during denoising.
          </p>
          <p>
            Here, we will follow the
            <a href="https://sde-image-editing.github.io/">SDEdit</a> algorithm
            to perform such image-to-image translation. The idea is we first add
            some noise to the original image (by <code>forward</code>), and then
            denoise it back with the diffusion model without extra text
            conditioning (i.e., via
            <code>iterative_denoise_cfg</code> conditioned on "a high quality
            photo"). The reconstructed image then becomes an edited version of
            the original image, which may look like the original image, but with
            some details "edited".
          </p>
          <h4>Implementation & Results</h4>
          <p>
            Conditioned on the prompt "a high quality photo" and with CFG scale
            of 7, here we edit the Campanile image and 2 cat images at noise
            levels of <code>i_start = [1, 3, 5, 7, 10, 20]</code>. Since the
            higher <code>i_start</code>, the less noise is added, hence, the
            edited image will look more similar to the original image. The
            results are shown below.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7_sdedit_campanile_seed100.png"
                data-full="media/part-a/a1-7_sdedit_campanile_seed100.png"
                alt="Image-to-Image Translation Results: Campanile"
              />
            </div>
            <figcaption>
              SDEdit on Campanile at
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 100)
            </figcaption>
          </figure>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7_sdedit_cat1_seed100.png"
                data-full="media/part-a/a1-7_sdedit_cat1_seed100.png"
                alt="Image-to-Image Translation Results: Cat 1"
              />
            </div>
            <figcaption>
              SDEdit on Cat 1 at
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 100)
            </figcaption>
          </figure>

          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7_sdedit_cat2_seed100.png"
                data-full="media/part-a/a1-7_sdedit_cat2_seed100.png"
                alt="Image-to-Image Translation Results: Cat 2"
              />
            </div>
            <figcaption>
              SDEdit on Cat 2 at
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 100)
            </figcaption>
          </figure>
        </section>

        <section id="part-A-1-7-1">
          <h4><u>A.1.7.1: Editing Hand-Drawn and Web Images</u></h4>
          <p>
            This SDEdit approach works particularly well for starting from
            nonrealistic images (e.g., paintings, sketches, some scribbles) and
            project them onto the natural image manifold.
          </p>
          <h4>Implementation & Results</h4>
          <p>
            Let's try applying the same procedure on one web image (a rose) and
            two of my hand-drawings created in Procreate (the Campanile and a
            panda). Everything is the same as in Part A.1.7, except that the
            starting images are different. The results are shown below.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7-1_sdedit_webimage_seed100.png"
                data-full="media/part-a/a1-7-1_sdedit_webimage_seed100.png"
                alt="Image-to-Image Translation Results: Web Image"
              />
            </div>
            <figcaption>
              SDEdit on Web Image at
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 100)
            </figcaption>
          </figure>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7-1_sdedit_campanile_sketch_seed100.png"
                data-full="media/part-a/a1-7-1_sdedit_campanile_sketch_seed100.png"
                alt="Image-to-Image Translation Results: Campanile Sketch"
              />
            </div>
            <figcaption>
              SDEdit on Campanile Sketch at
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 100)
            </figcaption>
          </figure>

          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7-1_sdedit_panda_sketch_seed100.png"
                data-full="media/part-a/a1-7-1_sdedit_panda_sketch_seed100.png"
                alt="Image-to-Image Translation Results: Panda Sketch"
              />
            </div>
            <figcaption>
              SDEdit on Panda Sketch at
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 100)
            </figcaption>
          </figure>
          <p>
            It can be observed that adding little noise (e.g.,
            <code>i_start = 20</code>) helps to refine the images while
            preventing most of the original structure from being lost, while
            adding more noise will yield somewhat the same general structure but
            quite different details. However, if we add too much noise, the
            results will be off the original image (particularly when
            <code>i_start = [1, 3]</code>) too much, even not have a similar
            structure.
          </p>
        </section>

        <section id="part-A-1-7-2">
          <h4><u>A.1.7.2: Inpainting</u></h4>
          <p>
            We can also use the same procedure to perform image inpainting
            (following the
            <a href="https://arxiv.org/abs/2201.09865">RePaint</a> paper).
          </p>
          <p>
            The inpainting process uses a binary mask $\mathbf{m}$ and applies
            it to the original image $x_{orig}$, and creates a new image that
            has the same content where $\mathbf{m} = 0$ and new content where
            $\mathbf{m} = 1$. This can be achieved by "forcing" the obtained
            $x_t$ at every step in the denoising loop to have the same pixels as
            $x_{orig}$ where $\mathbf{m} = 0$, i.e, $$ x_t \gets \mathbf{m} x_t
            + (1 - \mathbf{m}) \texttt{forward}(x_{orig},t).\tag{A.5}$$
          </p>
          <h4>Implementation & Results</h4>
          <p>
            The implementation of the <code>inpaint</code> function is shown
            below, which modifies the denoising loop in
            <code>iterative_denoise_cfg</code> to apply the mask at every step;
            the modified part is marked accordingly.
          </p>
          <figure class="code-card">
            <p>
              <em>Code Snippet:</em>
              implement <code>inpaint</code>
            </p>
            <pre><code class="language-python">def inpaint(original_image, mask, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  # ==== my modifications ====
  image = torch.randn_like(original_image).to(device).half()
  original_image = original_image.to(device).half()
  mask = mask.to(device).half()
  prompt_embeds = prompt_embeds.to(device).half()
  uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()
  # ==== end of modifications ====

  # use your previous `iterative_denoise_cfg` function and make the appropriate changes
  with torch.no_grad():
    # ==== new code for range ====
    for i in range(len(timesteps) - 1):
    # ==== end of new code ====
      # Get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # ==== new code for inpainting ====
      x_orig_forward = forward(original_image, t)
      image = mask * image + (1 - mask) * x_orig_forward
      # ==== end of new code ====

      # Get `alpha_cumprod`, `alpha_cumprod_prev`, `alpha`, `beta`
      alpha_cumprod = alphas_cumprod[t].to(image.device).type(image.dtype)
      alpha_cumprod_prev = alphas_cumprod[prev_t].to(image.device).type(image.dtype)
      alpha = alpha_cumprod / alpha_cumprod_prev
      beta = 1 - alpha

      # Get cond noise estimate
      model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt_embeds,
          return_dict=False
      )[0]

      # Get uncond noise estimate
      uncond_model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds,
          return_dict=False
      )[0]

      # Split estimate into noise and variance estimate
      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)
      uncond_noise_est, _ = torch.split(uncond_model_output, image.shape[1], dim=1)

      # Compute the CFG noise estimate based on equation 4
      epsilon_u = uncond_noise_est
      epsilon_c = noise_est
      epsilon = epsilon_u + scale * (epsilon_c - epsilon_u)

      # Get `pred_prev_image`, the next less noisy image.
      # compute `x_0` using one-step denoising
      x_t = image
      w_xt = 1 / torch.sqrt(alpha_cumprod)
      w_sigma = torch.sqrt((1 - alpha_cumprod) / (alpha_cumprod))
      x_0 = w_xt * x_t - w_sigma * epsilon # use cfg epsilon here
      
      # compute `pred_prev_image` (x_{t'})
      w_x0 = (torch.sqrt(alpha_cumprod_prev) * beta) / (1 - alpha_cumprod)
      w_xt = (torch.sqrt(alpha) * (1 - alpha_cumprod_prev)) / (1 - alpha_cumprod)
      xt_prev = w_x0 * x_0 + w_xt * x_t
      pred_prev_image = add_variance(predicted_variance, t, xt_prev)

      image = pred_prev_image

    clean = image.cpu().detach().numpy()

    return clean</code></pre>
          </figure>
          <p>
            With the above function, let's try inpainting on the Campanile and 2
            of my own images. For better results, I tried multiple seeds and
            picked the one that looks interesting to myself.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7-2_inpainting_campanile_seed38.png"
                data-full="media/part-a/a1-7-2_inpainting_campanile_seed38.png"
                alt="Image-to-Image Translation Results: Campanile Inpainting"
              />
            </div>
            <figcaption>
              Inpainting on Berkeley Campanile Spire
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 38)
            </figcaption>
          </figure>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7-2_inpainting_golden_gate_seed14.png"
                data-full="media/part-a/a1-7-2_inpainting_golden_gate_seed14.png"
                alt="Image-to-Image Translation Results: Golden Gate Inpainting"
              />
            </div>
            <figcaption>
              Inpainting on Golden Gate Bridge Tower
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 14)
            </figcaption>
          </figure>

          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7-2_inpainting_godfather_seed17.png"
                data-full="media/part-a/a1-7-2_inpainting_godfather_seed17.png"
                alt="Image-to-Image Translation Results: Godfather Inpainting"
              />
            </div>
            <figcaption>
              Inpainting on Vito Corleone's Suit
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 17)
            </figcaption>
          </figure>
        </section>

        <section id="part-A-1-7-3">
          <h4><u>A.1.7.3: Text-Conditional Image-to-image Translation</u></h4>
          <p>
            Instead of conditioning on a neutral prompt, we can also perform
            SDEdit guided by a specific text prompt, which no longer acts as a
            pure ‚Äúprojection‚Äù onto the natural image manifold, but adds
            controllability through language.
          </p>
          <h4>Implementation & Results</h4>
          <p>
            Let's do the text-conditional SDEdit on the Campanile and two of my
            selected images. With the increase of <code>i_start</code>, the
            edited images look more similar to the original images, while still
            incorporating elements from the text prompts. The prompts are shown
            in the captions
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7-3_sdedit_rocketship_campanile_seed100.png"
                data-full="media/part-a/a1-7-3_sdedit_rocketship_campanile_seed100.png"
                alt="Image-to-Image Translation Results: Campanile SDEdit"
              />
            </div>
            <figcaption>
              Berkeley Campanile
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 100), prompt = "a
              rocket ship"
            </figcaption>
          </figure>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7-3_sdedit_stormtrooper_armor_seed100.png"
                data-full="media/part-a/a1-7-3_sdedit_stormtrooper_armor_seed100.png"
                alt="Image-to-Image Translation Results: Stormtrooper Armor SDEdit"
              />
            </div>
            <figcaption>
              Armor
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 100), prompt = "a
              stormtrooper from Star Wars"
            </figcaption>
          </figure>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/a1-7-3_sdedit_einstein_godfather_seed100.png"
                data-full="media/part-a/a1-7-3_sdedit_einstein_godfather_seed100.png"
                alt="Image-to-Image Translation Results: Einstein Godfather SDEdit"
              />
            </div>
            <figcaption>
              Godfather
              <code>i_start = [1,3,5,7,10,20]</code> (seed = 100), prompt =
              "Albert Einstein"
            </figcaption>
          </figure>
        </section>

        <section id="part-A-1-8">
          <h3><u>A.1.8: Visual Anagrams</u></h3>
          <p>
            In this part, we will implement the
            <a href="https://dangeng.github.io/visual_anagrams/"
              >Visual Anagrams</a
            >
            and create optical illusions with diffusion models. The idea is to
            generate an image that can be interpreted in two different ways
            before and after some orthogonal transformations (e.g., flipping,
            rotation, etc.).
          </p>
          <p>
            To achieve this, we will denoise an image $x_t$ at step $t$ normally
            with the prompt $p_1$, to obtain noise estimate $\epsilon_1$. But at
            the same time, we will apply the orthogonal transformation to $x_t$
            and denoise with the prompt $p_2$ to get noise estimate
            $\epsilon_2$, after which, we transform $\epsilon_2$ back to the
            original orientation. Finally, we can perform a reverse/denoising
            step with the averaged noise estimate. The full algorithm is:
            $$\epsilon_1 = \text{CFG of UNet}(x_t, t, p_1),\tag{A.6}$$
            $$\epsilon_2 = \text{transform_back}(\text{CFG of
            UNet}(\text{transform}(x_t), t, p_2)),\tag{A.7}$$ $$\epsilon =
            (\epsilon_1 + \epsilon_2) / 2,\tag{A.8}$$ where the UNet is the
            diffusion model UNet from before, $\text{transform}(\cdot)$ is the
            orthogonal transformation function and
            $\text{transform_back}(\cdot)$ is its inverse, and $p_1$ and $p_2$
            are two different text prompt embeddings, and $\epsilon$ is our
            final noise estimate.
          </p>
          <h4>Implementation & Results</h4>
          <p>
            There are multiple orthogonal transformations we can use, here I
            implement the $\text{flip}(\cdot)$ function that flips the image
            both horizontally and vertically, i.e., a 180-degree rotation. The
            code snippet of the modified denoising function is shown below.
          </p>
          <figure class="code-card">
            <p>
              <em>Code Snippet:</em>
              implement Visual Anagrams in
              <code>iterative_denoise_visual_anagrams</code>
            </p>
            <pre><code class="language-python">def make_flip_illusion(image, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  # initialize inputs
  image = image.half().to(device)
  # two prompt embeddings
  prompt_embeds_1, prompt_embeds_2 = prompt_embeds[:]
  prompt_embeds_1 = prompt_embeds_1.half().to(device)
  prompt_embeds_2 = prompt_embeds_2.half().to(device)
  uncond_prompt_embeds = uncond_prompt_embeds.half().to(device)

  with torch.no_grad():
    for i in range(i_start, len(timesteps)-1):
      # get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # get 'alpha_cumprod', 'alpha_cumprod_prev', 'alpha', 'beta'
      alpha_cumprod = alphas_cumprod[t].to(image.device).type(image.dtype)
      alpha_cumprod_prev = alphas_cumprod[prev_t].to(image.device).type(image.dtype)
      alpha = alpha_cumprod / alpha_cumprod_prev
      beta = 1 - alpha

      ## prompt 1 (no flip)
      # get cond noise estimate
      model_output_1 = stage_1.unet(
        image,
        t,
        encoder_hidden_states=prompt_embeds_1,
        return_dict=False
      )[0]
      noise_est_1, predicted_variance_1 = torch.split(model_output_1, image.shape[1], dim=1)

      # uncond noise estimate
      uncond_model_output_1 = stage_1.unet(
        image,
        t,
        encoder_hidden_states=uncond_prompt_embeds,
        return_dict=False
      )
      uncond_noise_est_1, _ = torch.split(uncond_model_output_1[0], image.shape[1], dim=1)

      ## prompt 2 (flipped)
      # flip image both horizontally and vertically
      image_flipped = torch.flip(image, dims=[2,3])
      # get cond noise estimate
      model_output_2 = stage_1.unet(
        image_flipped,
        t,
        encoder_hidden_states=prompt_embeds_2,
        return_dict=False
      )[0]
      noise_est_2, _ = torch.split(model_output_2, image.shape[1], dim=1)

      # uncond noise estimate
      uncond_model_output_2 = stage_1.unet(
        image_flipped,
        t,
        encoder_hidden_states=uncond_prompt_embeds,
        return_dict=False
      )
      uncond_noise_est_2, _ = torch.split(uncond_model_output_2[0], image.shape[1], dim=1)
      
      # compute the CFG noise estimates for both prompts
      epsilon_u1 = uncond_noise_est_1
      epsilon_u2 = uncond_noise_est_2
      epsilon_c1 = noise_est_1
      epsilon_c2 = noise_est_2
      epsilon_1 = epsilon_u1 + scale * (epsilon_c1 - epsilon_u1)
      epsilon_2 = epsilon_u2 + scale * (epsilon_c2 - epsilon_u2)
      # flip epsilon_2 back
      epsilon_2 = torch.flip(epsilon_2, dims=[2,3])
      # average the two epsilons
      epsilon = (epsilon_1 + epsilon_2) / 2.0

      # Get `pred_prev_image`, the next less noisy image.
      # compute `x_0` using one-step denoising
      x_t = image
      w_xt = 1 / torch.sqrt(alpha_cumprod)
      w_sigma = torch.sqrt((1 - alpha_cumprod) / (alpha_cumprod))
      x_0 = w_xt * x_t - w_sigma * epsilon # use cfg epsilon here
      
      # compute `pred_prev_image` (x_{t'})
      w_x0 = (torch.sqrt(alpha_cumprod_prev) * beta) / (1 - alpha_cumprod)
      w_xt = (torch.sqrt(alpha) * (1 - alpha_cumprod_prev)) / (1 - alpha_cumprod)
      xt_prev = w_x0 * x_0 + w_xt * x_t
      pred_prev_image = add_variance(predicted_variance_1, t, xt_prev)

      image = pred_prev_image

    clean = image.cpu().detach().numpy()

    return clean</code></pre>
          </figure>
          <p>
            Two example visual anagrams created with the above function are
            shown below. In both cases, the images can be interpreted as
            different scenes before and after a 180-degree rotation (animated as
            follows). They are sampled with <code>seed = 28</code> and
            <code>seed = 2</code>, respectively.
          </p>
          <div class="anagrams-grid">
            <div
              class="anagram-item"
              data-transform="rotate-180"
              data-interval="4000"
            >
              <div class="anagram-frame">
                <img
                  src="media/part-a/a1-8_flip_illusion_1_seed28.png"
                  alt="Visual Anagram 1"
                  class="zoomable"
                />
              </div>
              <div class="anagram-prompts">
                <p class="anagram-prompt">
                  an oil painting of people around a campfire
                </p>
                <p class="anagram-prompt">a photo of a volcano</p>
              </div>
            </div>
            <div
              class="anagram-item"
              data-transform="rotate-180"
              data-interval="4000"
            >
              <div class="anagram-frame">
                <img
                  src="media/part-a/a1-8_flip_illusion_2_seed2.png"
                  alt="Visual Anagram 2"
                  class="zoomable"
                />
              </div>
              <div class="anagram-prompts">
                <p class="anagram-prompt">
                  a watercolor painting of flower arrangements
                </p>
                <p class="anagram-prompt">a photo of a dress</p>
              </div>
            </div>
          </div>
        </section>

        <section id="part-A-1-9">
          <h3><u>A.1.9: Hybrid Images</u></h3>
          <p>
            From part A.1.8, we can see that playing with the noise estimates at
            each step can yield interesting visual illusions, such as visual
            anagrams through orthogonal transformations. Not limited in
            orthogonal transformations, here we will implement
            <a href="https://arxiv.org/abs/2404.11615">Factorized Diffusion</a>
            and create hybrid images similar to those in project 2, i.e., images
            whose interpretation changes as a function of viewing distance.
          </p>
          <p>
            To do this, we will follow almost the same procedure as in part
            A.1.8 but applying low- and high- pass filters to the two noise
            estimates respectively before fusing them via sum. The algorithm is
            as follows: $$\epsilon_1 = \text{CFG of UNet}(x_t, t,
            p_1),\tag{A.9}$$ $$\epsilon_2 = \text{CFG of UNet}(x_t, t,
            p_2),\tag{A.10}$$ $$\epsilon = f_{\text{lowpass}}(\epsilon_1) +
            f_{\text{highpass}}(\epsilon_2)\tag{A.11}.$$
          </p>
          <h4>Implementation & Results</h4>
          <p>
            In my implementation, I use Gaussian filters for both low- and
            high-pass filtering, with a <code>kernel_size = 33</code> and
            <code>sigma = 2</code> for the low-pass filter. The code snippet of
            the modified denoising function is shown below.
          </p>
          <figure class="code-card">
            <p>
              <em>Code Snippet:</em>
              implement Hybrid Images in
              <code>iterative_denoise_hybrid_images</code>
            </p>
            <pre><code class="language-python">def make_hybrids(image, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  # initialize inputs
  image = image.half().to(device)
  # two prompt embeddings
  prompt_embeds_1, prompt_embeds_2 = prompt_embeds[:]
  prompt_embeds_1 = prompt_embeds_1.half().to(device)
  prompt_embeds_2 = prompt_embeds_2.half().to(device)
  uncond_prompt_embeds = uncond_prompt_embeds.half().to(device)

  with torch.no_grad():
    for i in range(i_start, len(timesteps)-1):
      # get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # get 'alpha_cumprod', 'alpha_cumprod_prev', 'alpha', 'beta'
      alpha_cumprod = alphas_cumprod[t].to(image.device).type(image.dtype)
      alpha_cumprod_prev = alphas_cumprod[prev_t].to(image.device).type(image.dtype)
      alpha = alpha_cumprod / alpha_cumprod_prev
      beta = 1 - alpha

      ## prompt 1 (low freq)
      # get cond noise estimate
      model_output_1 = stage_1.unet(
        image,
        t,
        encoder_hidden_states=prompt_embeds_1,
        return_dict=False
      )[0]
      noise_est_1, predicted_variance_1 = torch.split(model_output_1, image.shape[1], dim=1)

      # uncond noise estimate
      uncond_model_output_1 = stage_1.unet(
        image,
        t,
        encoder_hidden_states=uncond_prompt_embeds,
        return_dict=False
      )
      uncond_noise_est_1, _ = torch.split(uncond_model_output_1[0], image.shape[1], dim=1)

      ## prompt 2 (high freq)
      # get cond noise estimate
      model_output_2 = stage_1.unet(
        image,
        t,
        encoder_hidden_states=prompt_embeds_2,
        return_dict=False
      )[0]
      noise_est_2, _ = torch.split(model_output_2, image.shape[1], dim=1)

      # uncond noise estimate
      uncond_model_output_2 = stage_1.unet(
        image,
        t,
        encoder_hidden_states=uncond_prompt_embeds,
        return_dict=False
      )
      uncond_noise_est_2, _ = torch.split(uncond_model_output_2[0], image.shape[1], dim=1)
      
      # compute the CFG noise estimates for both prompts
      epsilon_u1 = uncond_noise_est_1
      epsilon_u2 = uncond_noise_est_2
      epsilon_c1 = noise_est_1
      epsilon_c2 = noise_est_2
      epsilon_1 = epsilon_u1 + scale * (epsilon_c1 - epsilon_u1)
      epsilon_2 = epsilon_u2 + scale * (epsilon_c2 - epsilon_u2)
      # low-pass to epsilon_1 and high-pass to epsilon_2
      epsilon_1_low = TF.gaussian_blur(
        epsilon_1,
        kernel_size=33,
        sigma=2
      )

      epsilon_2_high = epsilon_2 - TF.gaussian_blur(
        epsilon_2,
        kernel_size=33,
        sigma=2
      )
      # combine two epsilons
      epsilon = epsilon_1_low + epsilon_2_high

      # Get `pred_prev_image`, the next less noisy image.
      # compute `x_0` using one-step denoising
      x_t = image
      w_xt = 1 / torch.sqrt(alpha_cumprod)
      w_sigma = torch.sqrt((1 - alpha_cumprod) / (alpha_cumprod))
      x_0 = w_xt * x_t - w_sigma * epsilon # use cfg epsilon here
      
      # compute `pred_prev_image` (x_{t'})
      w_x0 = (torch.sqrt(alpha_cumprod_prev) * beta) / (1 - alpha_cumprod)
      w_xt = (torch.sqrt(alpha) * (1 - alpha_cumprod_prev)) / (1 - alpha_cumprod)
      xt_prev = w_x0 * x_0 + w_xt * x_t
      pred_prev_image = add_variance(predicted_variance_1, t, xt_prev)

      image = pred_prev_image

    clean = image.cpu().detach().numpy()

    return clean</code></pre>
          </figure>
          <p>
            Two example hybrid images are shown and animated below. They are
            generated with <code>seed = 8</code> and <code>seed = 36</code>,
            respectively, and can be interpreted as different scenes when viewed
            up close versus from afar.
          </p>

          <div class="hybrids-grid">
            <div
              class="hybrid-item"
              data-transform="zoom-out"
              data-interval="4000"
            >
              <div class="hybrid-frame">
                <img
                  src="media/part-a/a1-9_hybrid_1_seed8.png"
                  alt="Hybrid Image 1"
                  class="zoomable"
                />
              </div>
              <div class="hybrid-prompts">
                <p class="hybrid-prompt">a photo of grand canyon</p>
                <p class="hybrid-prompt">Albert Einstein</p>
              </div>
            </div>

            <div
              class="hybrid-item"
              data-transform="zoom-out"
              data-interval="4000"
            >
              <div class="hybrid-frame">
                <img
                  src="media/part-a/a1-9_hybrid_2_seed36.png"
                  alt="Hybrid Image 2"
                  class="zoomable"
                />
              </div>
              <div class="hybrid-prompts">
                <p class="hybrid-prompt">a lightgraph of waterfalls</p>
                <p class="hybrid-prompt">a photo of a panda</p>
              </div>
            </div>
          </div>
        </section>

        <section id="part-A-bw1">
          <h3><u>B &amp; W: More visual anagrams!</u></h3>
          <p>
            There are much more orthogonal transformations that can be used to
            create visual anagrams. Here let's implement two more!
          </p>
          <h3><u>1. Approximated Skew Anagram</u></h3>
          <p>
            Here we implement the skew transformation to create visual anagrams.
            Since the true skew transformation that inplemented via homography
            is not "orthogonal" as it relies on interpolation, we instead
            implement an approximated version mentioned in the paper, i.e.,
            <em>skewing by columns of pixels by different displacements</em>.
            This column-roll skew preserves pixel values and acts as a
            permutation matrix, thus satisfying the orthogonality constraint
            required by the noise-preserving property of diffusion models.
          </p>
          <h4>Implementation & Results</h4>
          <p>
            The implementation of this approximated skew transformation is
            straightforward once we have our previous flip anagram code, i.e.,
            just replace the flip operation with the skew operation. Here I'm
            not repeating the full code for
            <code>make_skew_illusion</code> function, but just show the skew
            operation code snippet below.
          </p>
          <figure class="code-card">
            <p>
              <em>Code Snippet:</em>
              implement skew operation in
              <code>make_skew_illusion</code>
            </p>
            <pre><code class="language-python">def skew_img(img, max_disp):
    b, c, h, w = img.shape
    # shifts for each column
    shifts = torch.linspace(-max_disp, max_disp, steps=w, device=img.device).round().long()  # (w,)
    # raw row indices of all pixels
    row_idx = torch.arange(h, device=img.device).view(h, 1) # (h, 1)
    # shifted row indices
    shifted_row_idx = (row_idx - shifts) % h # (h, w)
    shifted_row_idx = shifted_row_idx.view(1, 1, h, w).expand(b, c, h, w) # (b, c, h, w)
    # gather all pixels for skewed image
    skewed_img = torch.gather(img, 2, shifted_row_idx)
    return skewed_img</code></pre>
          </figure>
          <p>
            To use it, we just need to defined a <code>max_disp</code> as our
            maximum column displacement (in pixels) as an extra argument to the
            illusion function, then call the above
            <code>skew_img(im, max_disp)</code> as our transformation, and
            <code>skew_img(im, -max_disp)</code> as its inverse. The results of
            one example skew anagrams are shown below, with
            <code>max_disp = 50</code> on a $64 \times 64$ image. The image was
            sampled with <code>seed = 82</code>.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/bw_skew_illusion_seed82.png"
                data-full="media/part-a/bw_skew_illusion_seed82.png"
                alt="Skew Anagram"
              />
            </div>
          </figure>

          <h3><u>2. Negative Anagram</u></h3>
          <p>
            Not limited to geometric permutations, the color inversion like
            negatives can also be used as the orthogonal transformation, as it
            is intuitively a $180$ degree rotation generalized to higher
            dimensions, which allows us to generate illusions that change
            appearance upon color inversion, assuming pixel values are centered
            at $0$ (i.e., in $[-1, 1]$ range). Here we implement such negative
            anagram.
          </p>
          <h4>Implementation & Results</h4>
          <p>
            The implementation of negative anagram is even simpler. Everything
            is the same as our flip anagram code, except that the transformation
            function now is <code>negative_img</code>, defined as below, whose
            forward and inverse operations are the same.
          </p>
          <figure class="code-card">
            <p>
              <em>Code Snippet:</em>
              implement negative operation in
              <code>make_negative_illusion</code>
            </p>
            <pre><code class="language-python">def negative_img(img):
    return -img</code></pre>
          </figure>
          <p>
            One example negative anagram is shown below. The image was sampled
            with <code>seed = 68</code>.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/bw_negative_illusion_seed68.png"
                data-full="media/part-a/bw_negative_illusion_seed68.png"
                alt="Negative Anagram"
              />
            </div>
          </figure>
        </section>

        <section id="part-A-bw2">
          <h3><u>B &amp; W: Design a course logo!</u></h3>
          <p>
            I wanted to create a logo that a bear hidding behind a filter filled
            with some features of UC Berkeley. To do so, I sketched such a bear
            by myself in Procreate, and filling the $3 \times 3$ kernel with
            <a href="https://alumni.berkeley.edu/about-us/brand/color/"
              >colors representing UC Berkeley</a
            >. The original sketch is shown below.
          </p>
          <figure class="figure-center shrink-50">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/logo_sketch.png"
                data-full="media/part-a/logo_sketch.png"
                alt="Course Logo Sketch"
              />
              <figcaption>Raw Course Logo Sketch by Procreate</figcaption>
            </div>
          </figure>
          <p>
            Then, I used this sketch as the input image to perform
            text-conditional image-to-image translation to refine it. The prompt
            I used is "A clean, minimalistic course logo of the computer vision
            course of the University of California at Berkeley. A cute cartoon
            bear peeks from behind a square sign. Inside the square is a simple
            3x3 convolution kernel diagram. Smooth bold outlines, flat look,
            friendly style". Then, with <code>seed = 9</code>,
            <code>i_start = 25</code> and <code>scale = 7</code>, I obtained the
            final logo as shown below, which, although it somehow changed the
            original color but looks more professional and appealing, at least
            aligns my expectation :D.
          </p>
          <figure class="figure-center shrink-50">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-a/bw_logo_seed9.png"
                data-full="media/part-a/bw_logo_seed9.png"
                alt="Course Logo Final"
              />
              <figcaption>
                Course Logo after Text-Conditional Image-to-Image Translation
              </figcaption>
            </div>
          </figure>
        </section>
        <hr />

        <section id="part-B">
          <h2><u>Part B: Flow Matching from Scratch!</u></h2>
          <p>
            As we have explored diffusion models extensively in part A, now
            let's dive into
            <a href="https://arxiv.org/abs/2210.02747">flow matching</a> models
            and implement our own for image generation from scratch! Again, the
            implementation will follow this
            <a
              href="https://colab.research.google.com/drive/1GqpAzvLuPwYiwJaY0xLEqdx5IkBNqk1B?usp=drive_link"
              >provided notebook</a
            >, and we will play with
            <a
              href="https://docs.pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html"
              >MNIST</a
            >
            dataset for simplicity.
          </p>
        </section>

        <section id="part-B-1">
          <h3><u>B.1: Training a Single-Step Denoising UNet</u></h3>
          <o>
            Before jumping into flow matching, let's warmup by building a simple
            one-step denosier $D_{\theta}$ such that maps a noisy image $z$ to a
            clean image $x$, which can be trained over the $\mathcal{L}_2$ loss
            between the denoised output and the ground truth clean image:
            $$\mathcal{L}(\theta) = \mathbb{E}_{z,x} \left[ \| D_{\theta}(z) - x
            \|_2^2 \right].\tag{B.1}$$
          </o>
        </section>

        <section id="part-B-1-1">
          <h3><u>B.1.1: Implementing the UNet</u></h3>
          <p>
            In this whole part B, we will implement and use
            <a href="https://arxiv.org/abs/1505.04597">UNet</a> as our backbone
            for denoising and flow matching, i.e., the $D_{\theta}$. This
            architecture encodes the input image into a latent representation
            and then decodes it back to the original image space, while
            incorporating skip connections to preserve both low and high
            frequency spatial information. The implemented UNet follows the
            architecture shown in the figure below, including its architecture
            details and atomic operations.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/unconditional_arch.png"
                data-full="media/part-b/unconditional_arch.png"
                alt="Unconditional UNet Architecture"
              />
              <figcaption>Unconditional UNet Architecture</figcaption>
            </div>
          </figure>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/atomic_ops_new.png"
                data-full="media/part-b/atomic_ops_new.png"
                alt="Standard UNet Atomic Operations"
              />
              <figcaption>Standard UNet Atomic Operations</figcaption>
            </div>
          </figure>
        </section>

        <section id="part-B-1-2">
          <h3><u>B.1.2: Using the UNet to Train a Denoiser</u></h3>
          <p>
            Here we first train the $D_{\theta}$ as a one-step denoiser, i.e.,
            predicting the clean image $x$ from a noisy input $z$ directly,
            which can be done by minimizing the $\mathcal{L}_2$ loss mentioned
            in equation (B.1) above. The clean images are sampled from the MNIST
            training set ($x \sim \mathcal{D}_{\text{MNIST}}$), and the noise
            image $z$ is generated by adding Gaussian noise to $x$: $$z = x +
            \sigma \epsilon, \quad\text{where}\,\, \epsilon \sim \mathcal{N}(0,
            I),\tag{B.2}$$ where the $\sigma$ controls the noise level.
          </p>
          <h4>Visualization of the Noising Process</h4>
          <p>
            The visualization of the noising process with $\sigma = [0.0, 0.2,
            0.4, 0.5, 0.6, 0.8, 1.0]$ is shown below.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b1_2_noising_process.png"
                data-full="media/part-b/b1_2_noising_process.png"
                alt="Noising Process Visualization"
              />
              <figcaption>
                Noising Process Visualization with Varying Noise Levels
              </figcaption>
            </div>
          </figure>
        </section>

        <section id="part-B-1-2-1">
          <h4><u>B1.2.1 Training</u></h4>
          <p>
            We can now train our UNet denoiser to denoise the noisy images $z$
            generated by $\sigma=0.5$ from clean images $x$. The model is
            created following the architecture shown in B.1.1, optimized by
            Adam, and the hyperparameters are summarized in the following table.
          </p>
          <div class="hyperparam-table-wrapper">
            <div class="hyperparam-table-card">
              <div class="table-title">
                One-Step UNet Denoiser Training Setup
              </div>
              <div class="hyperparam-table-scroll">
                <table class="hyperparam-table">
                  <thead>
                    <tr>
                      <th><code>batch_size</code></th>
                      <th><code>learning_rate</code></th>
                      <th><code>noise_level</code></th>
                      <th><code>hidden_dim</code></th>
                      <th><code>num_epochs</code></th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td data-label="Batch">256</td>
                      <td data-label="LR">1e-4</td>
                      <td data-label="Noise Level">0.5</td>
                      <td data-label="Hidden Dim">128</td>
                      <td data-label="Epochs">5</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
          </div>

          <h4>Training Curve & Sample Results</h4>
          <p>
            The training loss curve is shown below, indicating that the model
            converges very fast and stably. Setting the same level of noise
            ($\sigma=0.5$), the denoised results on some test samples after
            checkpointing the model at different epochs are shown as well.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b1_2_1_training_curve.png"
                data-full="media/part-b/b1_2_1_training_curve.png"
                alt="Denoiser Training Curve"
              />
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b1_2_1_denoise_test_set.png"
                data-full="media/part-b/b1_2_1_denoise_test_set.png"
                alt="Denoised Results at Different Epochs"
              />
              <figcaption>Denoised Results at Different Epochs</figcaption>
            </div>
          </figure>
        </section>

        <section id="part-B-1-2-2">
          <h4><u>B1.2.2 Out-of-Distribution Testing</u></h4>
          <p>
            Our denoiser was trained on fixed noise level $\sigma=0.5$. Here we
            test its generalization on other noise levels. The denoised results
            on noisy test samples with $\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8,
            1.0]$ are shown as follows.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b1_2_2_ood_testing.png"
                data-full="media/part-b/b1_2_2_ood_testing.png"
                alt="Out-of-Distribution Denoising Results"
              />
              <figcaption>
                Out-of-Distribution Denoising Results at Varying Noise Levels
              </figcaption>
            </div>
          </figure>
          <p>
            It is obvious that the denoiser performs well on noise levels close
            to the training level, while it gradually degrades when denoising on
            higher levels, which is expected as it did not learned to handle
            such high noise during training.
          </p>
        </section>

        <section id="part-B-1-2-3">
          <h4><u>B1.2.3 Denoising Pure Noise</u></h4>
          <p>
            Recall that to make the denoising model generative, as discussed in
            Part A, we can apply it to pure Gaussian noise and denoise it,
            hence, $z = \epsilon \sim \mathcal{N}(0, \mathbf{I})$ as input,
            instead of being generated from $x$ anymore.
          </p>
          <p>
            Now, let's try this with our one-step UNet denoiser. Here we follow
            exactly the same procedure as in B.1.2.1 to train a UNet, where the
            only change is that $z = \epsilon \sim \mathcal{N}(0, \mathbf{I})$
            as our input.
          </p>
          <h4>Results & Discussion</h4>
          <p>
            The training loss curve and some sampled denoised results from pure
            noise after different epochs are shown below.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b1_2_3_pure_noise_training_curve.png"
                data-full="media/part-b/b1_2_3_pure_noise_training_curve.png"
                alt="Denoiser Training Curve"
              />
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b1_2_3_pure_noise_denoising.png"
                data-full="media/part-b/b1_2_3_pure_noise_denoising.png"
                alt="Denoised Results at Different Epochs"
              />
              <figcaption>Denoised Results at Different Epochs</figcaption>
            </div>
          </figure>
          <p>
            It can be observed that all generated images sampled from pure noise
            look almost identical. This is because the input distribution $Z
            \sim \mathcal{N} (0, \mathbf{I})$ provides no additional information
            as the condition for the model to generate diverse outputs in the
            target distribution $X \sim \mathcal{D}_{\text{MNIST}}$, hence, we
            have $Z \perp X$.
          </p>
          <p>
            Ideally, if we train a model with the following MSE loss function:
            $$\mathcal{L} = \mathbb{E}_{Z, X} \left[ \| D_{\theta}(z) - x \|^2
            \right],$$ the optimal solution should be the conditional
            expectation of $X$ given $Z$: $$D_{\theta}^*(z) = \mathbb{E}[X|Z].$$
          </p>
          <p>
            However, since the independence between $Z$ and $X$ holds, we have:
            $$\mathbb{E}[X|Z] = \mathbb{E}[X].$$
          </p>
          <p>
            Therefore, the denoiser is a constant function for $Z$ that always
            outputs the mean of the target distribution $\mathbb{E}[X]$, i.e.,
            $$D_{\theta}^*(z): Z \to \mathbb{E}[X]\quad$$
          </p>
          <p>
            Since the MNIST digits are centered around the middle of the image
            and are in the same scale, the mean of the target distribution is a
            weighted average of all digits, where the shared pattern will be
            highlighted, resulting in the image shown above.
          </p>
        </section>

        <section id="part-B-2">
          <h3><u>B.2: Training a Flow Matching Model</u></h3>
          <p>
            We just saw that one-step denoising from pure noise fails to
            generate diverse new samples due to the limitation of the training
            objective. Instead, we need to gradually denoise from noise to data
            through multiple steps, as we did in Part A with diffusion models.
            Here, we will do so via
            <a href="https://arxiv.org/abs/2210.02747">flow matching</a>, where
            our UNet model will learn to predict the "flow" that transports
            noise to the clean data manifold over continuous time, i.e., an
            ordinary differential equation (ODE) describing the transformation
            from noise to data. Then, in the sampling stage, we can solve this
            ODE to generate a new realistic image $x_1$ from pure noise $x_0
            \sim \mathcal{N}(0, \mathbf{I}).$
          </p>
          <p>
            For iterative denoising, we need to define how intermediate noisy
            samples are constructed, where the simplest approach is the linear
            interpolation between noise and data in our training set: $$x_t =
            (1-t)x_0 + t x_1 \quad \text{where}\,\, x_0 \sim \mathcal{N}(0,
            \mathbf{I}), \,\, t \in [0,1].\tag{B.3}$$
          </p>
          <p>
            This is a vector field describing the position of a point $x_t$ at
            time $t$ relative to the clean data distribution $p_1(x_1)$ and the
            noisy data distribution $p_0(x_0)$. Ituitively, we see that for
            small $t$, we remain close to noise, while for larger $t$, we
            approach the clean distribution.
          </p>
          <p>
            The "flow" can be interpreted as the velocity of this vector field
            that describing how points move from noise to data over time, i.e.,
            $$u(x_t,t) = \frac{d}{d t} x_t = x_1 - x_0.\tag{B.4}$$
          </p>
          <p>
            Therefore, our aim is to learn a $u_t(x_t, t)$ with our UNet model
            that approximates this flow $u(x_t, t) = x_1 - x_0$ through the
            objective below: $$ \mathcal{L}(\theta) = \mathbb{E}_{x_0 \sim
            p_0(x_0), x_1 \sim p_1(x_1), t\sim \text{Unif}(0,1)} \left[ \|(x_1 -
            x_0) - u_{\theta}(x_t, t)\|^2_2 \right].\tag{B.5}$$
          </p>
        </section>

        <section id="part-B-2-1">
          <h3><u>B.2.1 Adding Time Conditioning to UNet</u></h3>
          <p>
            Since our flow matching model $u_{\theta}(x_t, t)$ now takes an
            extra time variable $t$ as input, we need to modify our previous
            UNet architecture to condition it on $t$. There are many ways to do
            this, and we will follow the one introduced below.
          </p>
          <figure class="figure-center">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/conditional_arch_fm.png"
                data-full="media/part-b/conditional_arch_fm.png"
                alt="Time Conditioning in UNet"
              />
              <figcaption>Time Conditioning in UNet</figcaption>
            </div>
          </figure>
          <p>
            The new operator <code>FCBlock</code> is designed to encode and
            inject the conditioning signals into the UNet.
          </p>
          <figure class="figure-center shrink-70">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/fc_long.png"
                data-full="media/part-b/fc_long.png"
                alt="FCBlock Architecture"
              />
              <figcaption>FCBlock Architecture</figcaption>
            </div>
          </figure>
          <p>
            With aforementioned new architectural design, our UNet can be
            conditioned on $t$ through the following pseudo-code:
          </p>
          <figure class="code-card">
            <p><em>Code Snippet:</em> Time conditioning in UNet on t</p>
            <pre><code class="language-python">fc1_t = FCBlock(...)
fc2_t = FCBlock(...)

# the t passed in here should be normalized to be in the range [0, 1]
t1 = fc1_t(t)
t2 = fc2_t(t)

# Follow diagram to get unflatten.
# Replace the original unflatten with modulated unflatten.
unflatten = unflatten * t1
# Follow diagram to get up1.
...
# Replace the original up1 with modulated up1.
up1 = up1 * t2
# Follow diagram to get the output.
...</code></pre>
          </figure>
        </section>

        <section id="part-B-2-2">
          <h3><u>B.2.2 Training the Time-Conditioned UNet</u></h3>
          <p>
            The training of the time-conditioned UNet flow matching model
            follows the architecture introduced in B.2.1, trained on the
            objective in equation (B.5) via the Adam optimizer. The exponential
            learning rate scheduler is used, where the gamma was set to
            $0.1^{(1.0/\texttt{num_epochs})}$. Key hyperparameters are
            summarized in the following table.
          </p>
          <div class="hyperparam-table-wrapper">
            <div class="hyperparam-table-card">
              <div class="table-title">
                Time-Conditioned UNet Flow Matching Training Setup
              </div>
              <div class="hyperparam-table-scroll">
                <table class="hyperparam-table">
                  <thead>
                    <tr>
                      <th><code>batch_size</code></th>
                      <th><code>learning_rate</code></th>
                      <th><code>hidden_dim</code></th>
                      <th><code>num_epochs</code></th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td data-label="Batch">64</td>
                      <td data-label="LR">1e-2</td>
                      <td data-label="Hidden Dim">64</td>
                      <td data-label="Epochs">10</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
          </div>
          <p>
            The training process follows Algorithm 1 shown below, and the
            training loss curve is also attached afterwards, indicating a stable
            convergence.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/algo1_t_only_fm.png"
                data-full="media/part-b/algo1_t_only_fm.png"
                alt="Flow Matching Training Algorithm"
              />
              <figcaption>
                Algorithm B.1. Training Time-Conditioned UNet
              </figcaption>
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_2_time_conditioned_training_curve.png"
                data-full="media/part-b/b2_2_time_conditioned_training_curve.png"
                alt="Time-Conditioned UNet Flow Matching Training Curve"
              />
              <figcaption>
                Time-Conditioned UNet Flow Matching Training Curve
              </figcaption>
            </div>
          </figure>
        </section>

        <section id="part-B-2-3">
          <h3><u>B.2.3 Sampling From the Time-Conditioned UNet</u></h3>
          <p>
            We can now sample new images from pure noise by iteratively solving
            the ODE defined by our trained UNet flow matching model. The
            sampling procedure follows Algorithm 2 shown below, which is a
            simple Euler method with fixed step size $1/T$, and here we set
            $T=50$.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/algo2_t_only_fm.png"
                data-full="media/part-b/algo2_t_only_fm.png"
                alt="Flow Matching Sampling Algorithm"
              />
              <figcaption>
                Algorithm B.2. Sampling from Time-Conditioned UNet
              </figcaption>
            </div>
          </figure>
          <h4>Sampling Results</h4>
          <p>
            The sampled new images at checkpoints of epochs $1$, $5$, and $10$
            are shown below. It is obvious that the model gradually learns to
            generate more realistic and diverse digits as training proceeds.
            However, there are still some artifacts that make them far from
            perfect, though they are resonably good for only $10$ epochs of
            training.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_2_time_conditioned_epoch_1_samples.png"
                data-full="media/part-b/b2_2_time_conditioned_epoch_1_samples.png"
                alt="Sampled Images at Different Training Epoch 1"
              />
              <figcaption>
                Sampled Images at Different Training Epoch 1
              </figcaption>
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_2_time_conditioned_epoch_5_samples.png"
                data-full="media/part-b/b2_2_time_conditioned_epoch_5_samples.png"
                alt="Sampled Images at Different Training Epoch 5"
              />
              <figcaption>
                Sampled Images at Different Training Epoch 5
              </figcaption>
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_2_time_conditioned_epoch_10_samples.png"
                data-full="media/part-b/b2_2_time_conditioned_epoch_10_samples.png"
                alt="Sampled Images at Different Training Epoch 10"
              />
              <figcaption>
                Sampled Images at Different Training Epoch 10
              </figcaption>
            </div>
          </figure>
        </section>

        <section id="part-2-4">
          <h3><u>B.2.4 Adding Class-Conditioning to UNet</u></h3>
          <p>
            To make the results better and give us more control over the generation process, we can further condition our UNet model on class labels, i.e., the model now becomes $u_{\theta}(x, t, c)$ where $c$ is the class label.
          </p>
          <p>
            In practice, we can add 2 more <code>FCBlock</code> to our UNet, where the class conditioning vector $c$ will first be represented as a one-hot vector and then passed into these <code>FCBlock</code> modules to be embeded and injected into the UNet. To make our model able to work without being conditioned on class labels, which is important to implement classifier-free guidance during sampling, we randomly drop the class conditioning vector $c$ to be an all-zero vector with a probability of $10\%$ during training, hence, $p_{uncond} = 0.1$. The peudo-code for conditioning our UNet on both time and class labels is shown below.
          </p>
          <figure class="code-card">
            <p><em>Code Snippet:</em> Time and class conditioning in UNet on t and c</p>
            <pre><code class="language-python">fc1_t = FCBlock(...)
fc1_c = FCBlock(...)
fc2_t = FCBlock(...)
fc2_c = FCBlock(...)

t1 = fc1_t(t)
c1 = fc1_c(c)
t2 = fc2_t(t)
c2 = fc2_c(c)

# Follow diagram to get unflatten.
# Replace the original unflatten with modulated unflatten.
unflatten = c1 * unflatten + t1
# Follow diagram to get up1.
...
# Replace the original up1 with modulated up1.
up1 = c2 * up1 + t2
# Follow diagram to get the output.
...</code></pre>
          </figure>
        </section>

        <section id="part-B-2-5">
          <h3><u>B.2.5 Training the Class-Conditioned UNet</u></h3>
          <p>
            The training for this UNet with extra class conditioning follows almost the same procedure as the time-conditioned only one, where the only difference is to add the conditioning vector $c$ and do unconditional generation periodically, following Algorithm B.3 shown below.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/algo3_c_fm.png"
                data-full="media/part-b/algo3_c_fm.png"
                alt="Class-Conditioned Flow Matching Training Algorithm"
              />
              <figcaption>
                Algorithm B.3. Training Class-Conditioned UNet
              </figcaption>
            </div>
          </figure>
          With the same hyperparameters setting as in B.2.2, the training loss curve for this class-conditioned UNet is shown as follows.
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_5_class_conditioned_training_curve.png"
                data-full="media/part-b/b2_5_class_conditioned_training_curve.png"
                alt="Class-Conditioned UNet Flow Matching Training Curve"
              />
              <figcaption>
                Class-Conditioned UNet Flow Matching Training Curve
              </figcaption>
            </div>
          </figure>
        </section>

        <section id="part-B-2-6">
          <h3><u>B.2.6 Sampling with Class-Conditioned UNet</u></h3>
          <p>
            We can now sample new images with our class-conditioned UNet flow matching model. With this class conditioning, we can further apply CFG during sampling to enhance the generation quality. The sampling procedure follows Algorithm B.4 shown below, where the guidance scale $\gamma$ is set to $5.0$ in our experiments.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/algo4_c_fm.png"
                data-full="media/part-b/algo4_c_fm.png"
                alt="Class-Conditioned Flow Matching Sampling Algorithm"
              />
              <figcaption>
                Algorithm B.4. Sampling from Class-Conditioned UNet with CFG
              </figcaption>
            </div>
          </figure>
          <h4>Sampling Results</h4>
          <p>
            The sampled new images at checkpoints of epochs $1$, $5$, and $10$ are shown below. It is obvious that the model generates more realistic and diverse digits with class conditioning and CFG, compared to the time-conditioned only model.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_6_class_conditioned_epoch_1_samples.png"
                data-full="media/part-b/b2_6_class_conditioned_epoch_1_samples.png"
                alt="Sampled Images at Different Training Epoch 1"
              />
              <figcaption>
                Sampled Images at Different Training Epoch 1
              </figcaption>
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_6_class_conditioned_epoch_5_samples.png"
                data-full="media/part-b/b2_6_class_conditioned_epoch_5_samples.png"
                alt="Sampled Images at Different Training Epoch 5"
              />
              <figcaption>
                Sampled Images at Different Training Epoch 5
              </figcaption>
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_6_class_conditioned_epoch_10_samples.png"
                data-full="media/part-b/b2_6_class_conditioned_epoch_10_samples.png"
                alt="Sampled Images at Different Training Epoch 10"
              />
              <figcaption>
                Sampled Images at Different Training Epoch 10
              </figcaption>
            </div>
          </figure>
          <h4><u>Getting Rid of Learning Rate Scheduler</u></h4>
          <p>
            Here comes a question: can we get rid of the annoying learning rate scheduler? The answer is yes! From the training loss curve above, we can see that the loss actually decreases quite quickly at the beginning, which indicates that the scheduler is actually not that necessary. Therefore, for simplicity, we can fix the learning rate to a smaller value, e.g., $5 \times 10^{-3}$, and train the model longer for $15$ epochs without the scheduler. The training loss curve for this new setting is shown below, which is quite similar to the previous one with the scheduler.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_6_class_conditioned_no_scheduler_training_curve.png"
                data-full="media/part-b/b2_6_class_conditioned_no_scheduler_training_curve.png"
                alt="Class-Conditioned UNet Flow Matching Training Curve without Scheduler"
              />
              <figcaption>
                Class-Conditioned UNet Flow Matching Training Curve without Scheduler
              </figcaption>
            </div>
          </figure>
          <p>
            Also, let's sample some new images via this model at the last epoch and compare with the previous one with the scheduler, shown below. From the sampled results, we can not visually tell which one is better, meaning that fixing a smaller learning rate and training longer without the scheduler works just as well as using the scheduler.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/b2_6_class_conditioned_no_scheduler_epoch_15_samples.png"
                data-full="media/part-b/b2_6_class_conditioned_no_scheduler_epoch_15_samples.png"
                alt="Sampled Images at Training Epoch 15 without Scheduler"
              />
              <figcaption>
                Sampled Images at Training Epoch 15 without Scheduler
              </figcaption>
            </div>
          </figure>
        </section>

        <section id="part-B-bw">
          <h3><u>B &amp; W: Better Time-Conditioned UNet</u></h3>
          <p>
            Recall that our time-conditioning only UNet in B.2.3 can even though generate digits from Gaussian noise, but the quality is not as good as the class-conditioned one, e.g., there are many artifacts making the digits hard to recognize. Therefore, here we try to improve it!
          </p>
          <p>
            Since in this case we cannot apply CFG during sampling to enhance the generation quality, we instead enhance the model performance by improving its capacity of representation. For instance, here I increase the hidden dimension from $64$ to $128$. In the training stage, I also adjusted the initial learning rate to $5 \times 10^{-2}$, and increased the number of training epochs to $50$. The hyperparameters are summarized in the following table.
          </p>
          <div class="hyperparam-table-wrapper">
            <div class="hyperparam-table-card">
              <div class="table-title">
                Better Time-Conditioned UNet Flow Matching Training Setup
              </div>
              <div class="hyperparam-table-scroll">
                <table class="hyperparam-table">
                  <thead>
                    <tr>
                      <th><code>batch_size</code></th>
                      <th><code>learning_rate</code></th>
                      <th><code>hidden_dim</code></th>
                      <th><code>num_epochs</code></th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td data-label="Batch">64</td>
                      <td data-label="LR">5e-2</td>
                      <td data-label="Hidden Dim">128</td>
                      <td data-label="Epochs">50</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
          </div>
          <p>
            The training loss curve for this enhanced model is shown below, and the sampled new images at the last epoch are also attached afterwards.
          </p>
          <figure class="figure-center shrink-80">
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/bw_better_time_conditioned_training_curve.png"
                data-full="media/part-b/bw_better_time_conditioned_training_curve.png"
                alt="Better Time-Conditioned UNet Flow Matching Training Curve"
              />
              <figcaption>
                Better Time-Conditioned UNet Flow Matching Training Curve
              </figcaption>
            </div>
            <div>
              <img
                class="responsive zoomable"
                src="media/part-b/bw_better_time_conditioned_last_epoch_samples.png"
                data-full="media/part-b/bw_better_time_conditioned_last_epoch_samples.png"
                alt="Sampled Images at Training The Last Epoch"
              />
              <figcaption>Sampled Images at Training The Last Epoch</figcaption>
            </div>
          </figure>
          <p>
            It can be observed that, even though the quality is still not as good as the class-conditioned model (due to the lack of class information and CFG), the generated digits are much clearer and distinguishable than before, demonstrating the effectiveness of increasing model capacity and training time.
          </p>
        </section>

        <footer>
          <p>
            ¬© 2025 ZHOU Guanren |
            <a href="mailto:guanren_zhou@berkeley.edu">üìß Email</a> |
            <a
              href="https://github.com/Nutlettt"
              target="_blank"
              rel="noopener noreferrer"
            >
              <img
                src="../media/github.png"
                alt="GitHub"
                style="
                  width: 16px;
                  height: 16px;
                  vertical-align: middle;
                  margin-right: 4px;
                "
              />
              GitHub
            </a>
          </p>
        </footer>
      </div>
    </main>

    <div class="lightbox" id="pg-lightbox">
      <div class="lb-backdrop"></div>
      <div class="lb-inner">
        <img id="lb-img" alt="" />
        <div class="lb-caption" id="lb-cap"></div>
        <button class="lb-close" id="lb-close" aria-label="Close">
          &times;
        </button>
      </div>
    </div>
    <script>
      (function () {
        const sidebar = document.getElementById("sidebar");
        const overlay = document.getElementById("sidebar-overlay");
        const sidebarBtn = document.getElementById("sidebar-toggle");
        function closeSidebar() {
          sidebar.classList.add("collapsed");
          overlay.classList.remove("open");
        }
        function toggleSidebar() {
          const collapsed = sidebar.classList.toggle("collapsed");
          overlay.classList.toggle("open", !collapsed);
        }
        sidebarBtn && sidebarBtn.addEventListener("click", toggleSidebar);
        overlay && overlay.addEventListener("click", closeSidebar);
        document.addEventListener("keydown", (e) => {
          if (e.key === "Escape") closeSidebar();
        });

        const themeBtn = document.getElementById("theme-toggle");
        const root = document.documentElement;
        const targets = [root, document.body];
        function applyTheme(t) {
          root.setAttribute("data-theme", t);
          document.body.setAttribute("data-theme", t);
          const darkOn = t === "dark";
          const classes = ["dark", "dark-theme", "theme-dark", "mode-dark"];
          classes.forEach((c) => {
            root.classList.toggle(c, darkOn);
            document.body.classList.toggle(c, darkOn);
          });
          const lightClasses = [
            "light",
            "light-theme",
            "theme-light",
            "mode-light",
          ];
          lightClasses.forEach((c) => {
            root.classList.toggle(c, !darkOn);
            document.body.classList.toggle(c, !darkOn);
          });
          // highlight.js theme swap
          const hlLink = document.getElementById("hljs-theme");
          if (hlLink) {
            hlLink.href = darkOn
              ? "https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/styles/github-dark.min.css"
              : "https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/styles/github.min.css";
          }
          localStorage.setItem("theme", t);
          if (themeBtn)
            themeBtn.title =
              "Switch to " + (darkOn ? "light" : "dark") + " theme";
        }
        const stored = localStorage.getItem("theme");
        applyTheme(stored || "light");

        themeBtn &&
          themeBtn.addEventListener("click", () => {
            const next =
              root.getAttribute("data-theme") === "dark" ? "light" : "dark";
            applyTheme(next);
          });

        window
          .matchMedia("(prefers-color-scheme: dark)")
          .addEventListener("change", () => {});

        // Persist scroll position across refreshes (minimize top flash)
        (function persistScroll() {
          const key = `scroll:${window.location.pathname}`;
          history.scrollRestoration = "manual";

          const getScroller = () =>
            document.querySelector("main.with-sidebar") ||
            document.scrollingElement;

          function restore() {
            const scroller = getScroller();
            if (!scroller) return;
            const saved = sessionStorage.getItem(key);
            if (!saved) return;
            const y = Number(saved) || 0;
            // Apply twice across frames to avoid layout race
            scroller.scrollTo(0, y);
            requestAnimationFrame(() => scroller.scrollTo(0, y));
          }

          function save() {
            const scroller = getScroller();
            if (!scroller) return;
            sessionStorage.setItem(key, String(scroller.scrollTop));
          }

          if (document.readyState !== "loading") {
            restore();
          } else {
            document.addEventListener("DOMContentLoaded", restore, {
              once: true,
            });
          }

          window.addEventListener("pageshow", restore);
          window.addEventListener("beforeunload", save);
          document.addEventListener("visibilitychange", () => {
            if (document.visibilityState === "hidden") save();
          });
        })();

        // Break out of small/at-boundary scroll containers on direction change
        (function enableScrollEscape() {
          const rootScroller =
            document.scrollingElement || document.documentElement;

          function canScroll(el, dy) {
            const style = getComputedStyle(el);
            const oy = style.overflowY;
            if (!(oy === "auto" || oy === "scroll")) return false;

            const max = el.scrollHeight - el.clientHeight;
            if (max <= 0) return false;

            const top = el.scrollTop;
            if (dy < 0 && top > 0) return true;
            if (dy > 0 && top < max) return true;
            return false;
          }

          document.addEventListener(
            "wheel",
            (e) => {
              if (
                e.defaultPrevented ||
                e.ctrlKey ||
                Math.abs(e.deltaY) < Math.abs(e.deltaX)
              )
                return;

              const dy = e.deltaY;
              let el = e.target;
              while (
                el &&
                el !== document.body &&
                el !== document.documentElement
              ) {
                if (canScroll(el, dy)) return; // inner scroller will consume it
                el = el.parentElement;
              }

              // No scrollable ancestor can consume this delta; route it to the page
              rootScroller.scrollBy({ top: dy, behavior: "auto" });
              e.preventDefault();
            },
            { passive: false, capture: true }
          );
        })();

        const lb = document.getElementById("pg-lightbox");
        if (lb) {
          const imgEl = document.getElementById("lb-img");
          const capEl = document.getElementById("lb-cap");
          const closeBtn = document.getElementById("lb-close");
          function open(src, cap) {
            imgEl.src = src;
            capEl.textContent = cap || "";
            lb.classList.add("open");
          }
          function close() {
            lb.classList.remove("open");
            imgEl.src = "";
          }
          document.querySelectorAll("img.zoomable").forEach((im) => {
            im.addEventListener("click", () =>
              open(im.getAttribute("data-full") || im.src, im.alt)
            );
          });
          lb.addEventListener("click", (e) => {
            if (e.target === lb || e.target.classList.contains("lb-backdrop"))
              close();
          });
          closeBtn && closeBtn.addEventListener("click", close);
          document.addEventListener("keydown", (e) => {
            if (e.key === "Escape" && lb.classList.contains("open")) close();
          });
        }
      })();
    </script>
    <script>
      window.addEventListener("load", () => {
        if (!window.hljs) return;
        document.querySelectorAll("pre code").forEach((block) => {
          hljs.highlightElement(block);
        });
      });
    </script>
    <script src="anagrams.js"></script>
    <script src="hybrids.js"></script>
  </body>
</html>
