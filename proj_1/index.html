<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project 1: Images of the Russian Empire -- Colorizing the Prokudin-Gorskii Photo Collection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- keep global theme -->
  <link rel="stylesheet" href="../style.css">
  <!-- project-specific tweaks -->
  <link rel="stylesheet" href="proj_1.css">

  <!-- MathJax v3 (LaTeX in HTML) -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']]
      },
      options: {
        skipHtmlTags: ['script','noscript','style','textarea','pre','code']
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

  <!-- Top Navbar -->
  <nav class="top-navbar">
    <div class="navbar-left">
      <a class="breadcrumb" href="../index.html">CS280A</a>
      <span class="breadcrumb-sep">‚Ä∫</span>
      <span class="breadcrumb" aria-current="page">Project 1</span>
    </div>
    <div class="navbar-right">
      <button id="sidebar-toggle" class="top-icon-btn" aria-label="Toggle sidebar" title="Toggle sidebar">‚ò∞</button>
      <button id="theme-toggle" class="top-icon-btn" aria-label="Toggle color theme" title="Toggle color theme">üåì</button>
    </div>
  </nav>

  <!-- Left Sidebar -->
  <aside id="sidebar" class="sidebar collapsed">
    <nav>
      <ul>
        <li><a href="#introduction">Introduction<span class="sidebar-sub">Project overview</span></a></li>
        <li><a href="#small-jpg">Small JPG Alignment<span class="sidebar-sub">Exhaustive search</span></a></li>
        <li><a href="#large-tif">Large TIF Alignment<span class="sidebar-sub">Image pyramid</span></a></li>
        <li><a href="#bells-whistles">Bells &amp; Whistles<span class="sidebar-sub">Enhancements</span></a></li>
        <li><a href="#better-feature">B&amp;W ‚Äî Better Feature</a></li>
        <li><a href="#auto-cropping">B&amp;W ‚Äî Automatic Cropping</a></li>
        <li><a href="#auto-wb">B&amp;W ‚Äî Automatic White Balance</a></li>
        <li><a href="#auto-contrast">B&amp;W ‚Äî Automatic Contrast</a></li>
        <li><a href="#better-colormap">B&amp;W ‚Äî Better Color Map</a></li>
      </ul>
    </nav>
  </aside>
  <!-- overlay for sidebar -->
  <div class="overlay" id="sidebar-overlay" aria-hidden="true"></div>

  <!-- Content -->
  <main class="with-sidebar">
    <div class="main-center">
      <header>
        <h1>Images of the Russian Empire -- Colorizing the Prokudin-Gorskii Photo Collection</h1>
        <h4>Aligning RGB channels from glass plate negatives and enhancing them for better visual quality</h4>
      </header>

      <!-- Introduction -->
      <section id="introduction">
        <h2>Introduction</h2>
        <div class="intro-flex">
          <div class="intro-img">
            <img src="media/output/pyramid_sobel/self/aligned_(37, 176)_(29, 78)_(17.84).jpg" alt="Prokudin-Gorskii Self Portrait">
            <div class="img-cap">Prokudin-Gorskii Self Portrait</div>
          </div>
          <div class="intro-text">
            <p>
              <a href="https://en.wikipedia.org/wiki/Sergey_Prokudin-Gorsky" target="_blank" rel="noopener noreferrer">Sergei Mikhailovich Prokudin-Gorskii</a> (1863-1944) was a chemist and photographer, best known as a pioneer of color photography. With special permission from the Tzar, he traveled across Russia and neighboring regions carrying a custom-designed camera. Due to the technical limitations of that era, each so-called ‚Äúcolor photograph‚Äù was actually recorded as three separate negatives on a single specially shaped glass plate, each taken through a red, green, or blue filter. Fortunately, many of these high-quality negatives have been preserved and are now housed at the Library of Congress (LOC), which has digitized and publicly released them. As a result, we have the unique opportunity to glimpse the Russian Empire in color more than a century ago.
            </p>
          </div>
          <div class="intro-img">
            <img src="media/example/prokudin_plate.jpg" alt="Original RGB Plate">
            <div class="img-cap">Original RGB Plate</div>
          </div>
        </div>
        <p>In this project, we start from the digitized glass plates provided by the LOC, separate the three RGB channels, and align them to reconstruct plausible color images that Prokudin-Gorskii wanted to share. Different metrics and alignment algorithms are explored to achieve accurate and efficient alignment even for large high-resolution scans. In addition, a series of post-processing enhancement techniques, including automatic cropping, white balance, contrast adjustment, and color remapping, are applied to improve visual quality and produce more natural and historically faithful results.
        </p>
        <div class="discussion-card">
          <h3>Highlights</h3>
          <ul>
            <li>Exhaustive search for small JPGs using NCC.</li>
            <li>Image pyramid for large TIFs for better efficiency.</li>
            <li>Robust alignment via Sobel gradient-domain NCC.</li>
            <li>Automatic border cropping (edge density + k-part trick), white balance (white patch assumption with tricks), and contrast (CLAHE).</li>
            <li>Optional Lab color transfer for historical aesthetics.</li>
          </ul>
        </div>
      </section>
      <hr>

      <!-- Small JPG alignment -->
      <section id="small-jpg">
        <h2>Aligning Small JPG Images</h2>
        <h3>Metrics for Image Alignment</h3>
        <p>Before aligning the images, one needs to define a metric to quantify "how well the images are aligned." This can be achieved through measuring the similarity between two images (patches). With a reference image $P$ and a target image $I$ (that we want to align with $P$), we define a region/patch $N$ that we want to align, then, our goal is to quantify how similar the region $N$ in $I$ is to $P$ after shifting $I$ by $(u,v)$. Two simple metrics are provided:</p>
        <ul>
          <li><strong>Sum of Squared Differences (SSD) </strong>:the squared L2 norm between two images in the pixel/feature space,
            <div class="math-block">$$
            \text{SSD}(u,v) = \sum_{(x,y)\in N} \left[I(u+x, v+y) - P(x,y)\right]^2
            $$</div>
          </li>
          <li><strong><a href="https://xcdskd.readthedocs.io/en/latest/cross_correlation/cross_correlation_coefficient.html" target="_blank" rel="noopener noreferrer">Normalized Cross-Correlation (NCC)</a></strong>: the dot product between two images in the pixel/feature space, i.e., the cosine similarity between two images after subtracting their mean and normalization
            <div class="math-block">$$
            \text{NCC}(u,v) = 
            \frac{\sum_{(x,y)\in N}\left[I(u+x,v+y)-\bar I\right]\left[P(x,y)-\bar P\right]}
            {\sqrt{\sum_{(x,y)\in N}\left[I(u+x,v+y)-\bar I\right]^2}\sqrt{\sum_{(x,y)\in N}\left[P(x,y)-\bar P\right]^2}}
            $$</div>
          </li>
        </ul>
        <p>With a glance at the definitions above, it is apparent that NCC should behave more robustly than SSD, since it is less sensitive to global shift (via subtracting the mean) and scaling (by normalization), which are both general issues in our case because Prokudin-Gorskii's 3 channels were not taken simultaneously with the same exposure. Therefore, for our implementation, let's start with NCC.</p>

        <h3>Pre-Processing: Patching</h3>
        <p>Before applying the alignment algorithm, appropriate pre-processing is required to improve robustness. In particular, computing the similarity metric only on the central region of the image (e.g., the central $90\%$) helps to avoid boundary artifacts. Since metrics like SSD and NCC rely heavily on pixel intensities, including these border regions may significantly degrade their reliability. Therefore, we restrict the calculation to a central patch for more stable scoring.</p>

        <h3>Search Algorithm: Exhaustive Search</h3>
        <p>With the similarity metric and the search space (either in the raw pixel space or a transformed feature space) defined, the optimal alignment can be obtained by searching over possible shifts. Here, we adopt a simple exhaustive search in the raw pixel space. The implementation details are as follows:</p>
          <ol>
            <li>Search grid: allow shifts within $[-15, 15]$ pixels around the center $(0, 0)$ in both $x$ and $y$ directions</li>
            <li>Metric: use NCC, which is more robust than SSD as we discussed above.</li>
            <li>Scoring region: restrict metric computation to a central crop, ignoring approximately $10\%$ of the margin to reduce border artifacts</li>
            <li>Output: evaluate all candidate shifts in the grid and return the optimal $(dx, dy)$ for G and R relative to B</li>
          </ol>

        <h3>Results of Single-Scale Alignment (Exhaustive Search + NCC over Raw Pixel Space on Small JPG Images)</h3>
        <p>The results are presented below, following the order such that the shift of R channel $(dx, dy)$, shift of G channel $(dx, dy)$, and run time.</p>
        <div class="gallery gallery-wide">
          <div>
            <img class="responsive zoomable"
                 src="media/output/small_raw/monastery/aligned_(2, 3)_(2, -3)_(0.84).jpg"
                 data-full="media/output/small_raw/monastery/aligned_(2, 3)_(2, -3)_(0.84).jpg"
                 alt="R(2,3) G(2,-3) 0.84s">
            <p style="text-align:center; color:var(--text-muted); margin-top:6px;">
              R(2, 3) G(2, -3) 0.84s
            </p>
          </div>
          <div>
            <img class="responsive zoomable"
                 src="media/output/small_raw/tobolsk/aligned_(3, 6)_(2, 3)_(0.87).jpg"
                 data-full="media/output/small_raw/tobolsk/aligned_(3, 6)_(2, 3)_(0.87).jpg"
                 alt="R(3,6) G(2,3) 0.87s">
            <p style="text-align:center; color:var(--text-muted); margin-top:6px;">
              R(3, 6) G(2, 3) 0.87s
            </p>
          </div>
          <div>
            <img class="responsive zoomable"
                 src="media/output/small_raw/cathedral/aligned_(3, 12)_(2, 5)_(0.83).jpg"
                 data-full="media/output/small_raw/cathedral/aligned_(3, 12)_(2, 5)_(0.83).jpg"
                 alt="R(3,12) G(2,5) 0.83s">
            <p style="text-align:center; color:var(--text-muted); margin-top:6px;">
              R(3, 12) G(2, 5) 0.83s
            </p>
          </div>
        </div>
      </section>
      <hr>

      <!-- Large TIF alignment -->
      <section id="large-tif">
        <h2>Aligning Large TIF Images</h2>
        <h3>Image Pyramid for Faster Alignment</h3>
        <p>The exhaustive search algorithm works reasonably well for small images with limited search ranges. However, its computational cost grows dramatically with the search window and image resolution, which quickly becomes impractical. For instance, performing the exhaustive search over a shift range of $[-50, 50]$ pixels in both $x$ and $y$ directions on the raw TIF image (without parallelization) may take over 20 minutes!</p>
        <p>Therefore, more efficient methods should be considered. Here we implement a coarse-to-fine alignment with <a href="https://en.wikipedia.org/wiki/Pyramid_(image_processing)">image pyramid</a>, where for each level, an exhaustive search is performed, but only within a much smaller local search window.</p>
        <p>More specifically, each level is constructed by applying a blur followed by downsampling (by a factor of 2 in our implementation). Blurring is necessary before downsampling at each level, aiming to avoid <a href="https://en.wikipedia.org/wiki/Aliasing">aliasing</a>. The alignment starts from the coarsest level, and the optimal shift is propagated to the next finer level as the initial guess, until the finest level (the raw image) is reached. Therefore, except for the coarsest level, the searching space at each level is reduced significantly, which is much smaller than the original searching space of naive exhaustive search.</p>
        <p><strong>Implementation details include:</strong></p>
          <ol>
            <li>Build pyramids through "blur + downsampling by 2" (via <a href="https://opencv.org/blog/resizing-and-rescaling-images-with-opencv/">$\texttt{cv2.resize}$</a> interpolated by <a href="https://medium.com/@wenrudong/what-is-opencvs-inter-area-actually-doing-282a626a09b3">$\texttt{INTER_AREA}$</a>), until minimum size (e.g., 300 px).</li>
            <li>Start from the coarsest level with a wide search range (e.g., $[-20, 20]$) around the initial guess (e.g., the center $(0, 0)$), get the optimal shift.</li>
            <li>At finer levels, use the last level's optimal shift (scaled by the downsampling factor, e.g., 2) as the initial guess, then search over a smaller range (e.g., $[-3, 3]$) around it for the new optimal shift.</li>
            <li>Repeat step 3 until the finest level is reached, i.e., the raw image.</li>
          </ol>

        <h3>Results of Multi-Scale Alignment (Exhaustive Search + NCC over Raw Pixel Space on All Images)</h3>
        <div class="multi-align-grid">
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/cathedral/aligned_(3, 12)_(2, 5)_(1.52).jpg"
                   data-full="media/output/pyramid_raw/cathedral/aligned_(3, 12)_(2, 5)_(1.52).jpg"
                   alt="R(3,12) G(2,5) 1.52s">
            </div>
            <p>R(3,12) G(2,5) 1.52s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/church/aligned_(-5, 58)_(-12, 24)_(16.01).jpg"
                   data-full="media/output/pyramid_raw/church/aligned_(-5, 58)_(-12, 24)_(16.01).jpg"
                   alt="R(-5,58) G(-12,24) 16.01s">
            </div>
            <p>R(-5,58) G(-12,24) 16.01s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/emir/aligned_(55, 104)_(24, 49)_(16.40).jpg"
                   data-full="media/output/pyramid_raw/emir/aligned_(55, 104)_(24, 49)_(16.40).jpg"
                   alt="R(55,104) G(24,49) 16.40s">
            </div>
            <p>R(55,104) G(24,49) 16.40s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/harvesters/aligned_(14, 124)_(16, 60)_(16.39).jpg"
                   data-full="media/output/pyramid_raw/harvesters/aligned_(14, 124)_(16, 60)_(16.39).jpg"
                   alt="R(14,124) G(16,60) 16.39s">
            </div>
            <p>R(14,124) G(16,60) 16.39s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/icon/aligned_(23, 89)_(17, 40)_(16.56).jpg"
                   data-full="media/output/pyramid_raw/icon/aligned_(23, 89)_(17, 40)_(16.56).jpg"
                   alt="R(23,89) G(17,40) 16.56s">
            </div>
            <p>R(23,89) G(17,40) 16.56s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/italil/aligned_(35, 77)_(21, 38)_(16.58).jpg"
                   data-full="media/output/pyramid_raw/italil/aligned_(35, 77)_(21, 38)_(16.58).jpg"
                   alt="R(35,77) G(21,38) 16.58s">
            </div>
            <p>R(35,77) G(21,38) 16.58s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/lastochikino/aligned_(-9, 76)_(-2, -3)_(16.83).jpg"
                   data-full="media/output/pyramid_raw/lastochikino/aligned_(-9, 76)_(-2, -3)_(16.83).jpg"
                   alt="R(-9,76) G(-2,-3) 16.83s">
            </div>
            <p>R(-9,76) G(-2,-3) 16.83s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/lugano/aligned_(-29, 92)_(-17, 41)_(16.63).jpg"
                   data-full="media/output/pyramid_raw/lugano/aligned_(-29, 92)_(-17, 41)_(16.63).jpg"
                   alt="R(-29,92) G(-17,41) 16.63s">
            </div>
            <p>R(-29,92) G(-17,41) 16.63s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/melons/aligned_(12, 178)_(10, 82)_(16.82).jpg"
                   data-full="media/output/pyramid_raw/melons/aligned_(12, 178)_(10, 82)_(16.82).jpg"
                   alt="R(12,178) G(10,82) 16.82s">
            </div>
            <p>R(12,178) G(10,82) 16.82s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/monastery/aligned_(2, 3)_(2, -3)_(1.51).jpg"
                   data-full="media/output/pyramid_raw/monastery/aligned_(2, 3)_(2, -3)_(1.51).jpg"
                   alt="R(2,3) G(2,-3) 1.51s">
            </div>
            <p>R(2,3) G(2,-3) 1.51s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/self/aligned_(34, 175)_(29, 79)_(17.10).jpg"
                   data-full="media/output/pyramid_raw/self/aligned_(34, 175)_(29, 79)_(17.10).jpg"
                   alt="R(34,175) G(29,79) 17.10s">
            </div>
            <p>R(34,175) G(29,79) 17.10s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/siren/aligned_(-25, 96)_(-7, 49)_(17.23).jpg"
                   data-full="media/output/pyramid_raw/siren/aligned_(-25, 96)_(-7, 49)_(17.23).jpg"
                   alt="R(-25,96) G(-7,49) 17.23s">
            </div>
            <p>R(-25,96) G(-7,49) 17.23s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/three/aligned_(10, 112)_(13, 55)_(16.54).jpg"
                   data-full="media/output/pyramid_raw/three/aligned_(10, 112)_(13, 55)_(16.54).jpg"
                   alt="R(10,112) G(13,55) 16.54s">
            </div>
            <p>R(10,112) G(13,55) 16.54s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_raw/tobolsk/aligned_(3, 6)_(2, 3)_(1.54).jpg"
                   data-full="media/output/pyramid_raw/tobolsk/aligned_(3, 6)_(2, 3)_(1.54).jpg"
                   alt="R(3,6) G(2,3) 1.54s">
            </div>
            <p>R(3,6) G(2,3) 1.54s</p>
          </div>
        </div>
      </section>
      <hr>

      <!-- Bells & Whistles -->
      <section id="bells-whistles">
        <h2>Bells &amp; Whistles</h2>
        <h3>Failure Mode Analysis</h3>
        <p>The aforementioned baseline uses NCC over raw pixel space at each pyramid level with exhaustive search, can already achieve reasonable alignment. However, the processed images still contain some artifacts that makes them less visually appealing and not perfectly aligned. For instance, systematic failures can be observed in some TIF images, like the $\texttt{emir.tif}$, where the red channel is obviously mis-aligned (shown below). Therefore, here we analyze and address those systematic failures, and then apply some techniques to enhance the visual quality.</p>
        
      <!-- Better Feature -->
        <section id="better-feature">
          <h3>Better Alignment: Robust Metric and/or Feature Space</h3>
          <p>While the NCC is foundamentally better than SSD due to its normalization, applying it directly on the raw pixel space still makes it highly sensitive to absolute intensities. This results in poor robustness against local defects and outliers (e.g., saturation, scratches, noise), and leads to systematic failures when the channels differ significantly in contrast and brightness.</p>
          <p>In $\texttt{emir.tif}$, when we visualize the three channels separately (see below), one can easily observe that the R channel exhibits a highly different contrast and saturation distribution compared to B and G channels. Consequently, NCC on raw intensities tends to systematically mis-align the red channel.</p>
          <div class="bw-grid">
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/example/emir_r_channel_cmap.jpg"
                     data-full="media/example/emir_r_channel_cmap.jpg"
                     alt="R channel">
              </div>
              <p class="bw-cap">Red channel in red colormap</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/example/emir_g_channel_cmap.jpg"
                     data-full="media/example/emir_g_channel_cmap.jpg"
                     alt="G channel">
              </div>
              <p class="bw-cap">Green channel in green colormap</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/example/emir_b_channel_cmap.jpg"
                     data-full="media/example/emir_b_channel_cmap.jpg"
                     alt="B channel">
              </div>
              <p class="bw-cap">Blue channel in blue colormap</p>
            </div>
          </div>
          
          <p>To address this, one can:</p>
          <ol>
            <li>either adopt a more robust metric, or</li>
            <li>map the raw pixel space into feature spaces that are less sensitive to intensity variations.</li>
          </ol>
          <p>In my implementation, the latter strategy was adopted. Specifically, I compute NCC in the <a href="https://en.wikipedia.org/wiki/Sobel_operator">Sobel gradient space</a>, where the focus is shifted from pixel intensities to edges and structural information. And apparently, the misalignment issue was resolved appropriately.</p>
        <h3>Results of Multi-Scale Alignment (Exhaustive Search + NCC over Sobel Gradient Space on All Images)</h3>
        <div class="multi-align-grid">
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/cathedral/aligned_(3, 12)_(2, 5)_(1.52).jpg"
                   data-full="media/output/pyramid_sobel/cathedral/aligned_(3, 12)_(2, 5)_(1.52).jpg"
                   alt="R(3,12) G(2,5) 1.52s">
            </div>
            <p>R(3,12) G(2,5) 1.52s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/church/aligned_(-4, 58)_(4, 25)_(16.82).jpg"
                   data-full="media/output/pyramid_sobel/church/aligned_(-4, 58)_(4, 25)_(16.82).jpg"
                   alt="R(-4,58) G(4,25) 16.82s">
            </div>
            <p>R(-4,58) G(4,25) 16.82s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/emir/aligned_(40, 107)_(23, 49)_(16.77).jpg"
                   data-full="media/output/pyramid_sobel/emir/aligned_(40, 107)_(23, 49)_(16.77).jpg"
                   alt="R(40,107) G(23,49) 16.77s">
            </div>
            <p>R(40,107) G(23,49) 16.77s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/harvesters/aligned_(13, 123)_(17, 60)_(17.15).jpg"
                   data-full="media/output/pyramid_sobel/harvesters/aligned_(13, 123)_(17, 60)_(17.15).jpg"
                   alt="R(13,123) G(17,60) 17.15s">
            </div>
            <p>R(13,123) G(17,60) 17.15s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/icon/aligned_(23, 90)_(17, 42)_(17.62).jpg"
                   data-full="media/output/pyramid_sobel/icon/aligned_(23, 90)_(17, 42)_(17.62).jpg"
                   alt="R(23,90) G(17,42) 17.62s">
            </div>
            <p>R(23,90) G(17,42) 17.62s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/italil/aligned_(36, 77)_(22, 39)_(17.20).jpg"
                   data-full="media/output/pyramid_sobel/italil/aligned_(36, 77)_(22, 39)_(17.20).jpg"
                   alt="R(36,77) G(22,39) 17.20s">
            </div>
            <p>R(36,77) G(22,39) 17.20s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/lastochikino/aligned_(-8, 76)_(-2, -3)_(17.47).jpg"
                   data-full="media/output/pyramid_sobel/lastochikino/aligned_(-8, 76)_(-2, -3)_(17.47).jpg"
                   alt="R(-8,76) G(-2,-3) 17.47s">
            </div>
            <p>R(-8,76) G(-2,-3) 17.47s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/lugano/aligned_(-29, 91)_(-17, 41)_(17.71).jpg"
                   data-full="media/output/pyramid_sobel/lugano/aligned_(-29, 91)_(-17, 41)_(17.71).jpg"
                   alt="R(-29,91) G(-17,41) 17.71s">
            </div>
            <p>R(-29,91) G(-17,41) 17.71s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/melons/aligned_(12, 177)_(10, 80)_(17.73).jpg"
                   data-full="media/output/pyramid_sobel/melons/aligned_(12, 177)_(10, 80)_(17.73).jpg"
                   alt="R(12,177) G(10,80) 17.73s">
            </div>
            <p>R(12,177) G(10,80) 17.73s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/monastery/aligned_(2, 3)_(2, -3)_(1.50).jpg"
                   data-full="media/output/pyramid_sobel/monastery/aligned_(2, 3)_(2, -3)_(1.50).jpg"
                   alt="R(2,3) G(2,-3) 1.50s">
            </div>
            <p>R(2,3) G(2,-3) 1.50s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/self/aligned_(37, 176)_(29, 78)_(17.84).jpg"
                   data-full="media/output/pyramid_sobel/self/aligned_(37, 176)_(29, 78)_(17.84).jpg"
                   alt="R(37,176) G(29,78) 17.84s">
            </div>
            <p>R(37,176) G(29,78) 17.84s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/siren/aligned_(-24, 96)_(-7, 49)_(17.68).jpg"
                   data-full="media/output/pyramid_sobel/siren/aligned_(-24, 96)_(-7, 49)_(17.68).jpg"
                   alt="R(-24,96) G(-7,49) 17.68s">
            </div>
            <p>R(-24,96) G(-7,49) 17.68s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/three/aligned_(8, 111)_(12, 54)_(16.80).jpg"
                   data-full="media/output/pyramid_sobel/three/aligned_(8, 111)_(12, 54)_(16.80).jpg"
                   alt="R(8,111) G(12,54) 16.80s">
            </div>
            <p>R(8,111) G(12,54) 16.80s</p>
          </div>
          <div class="card">
            <div class="img-frame">
              <img class="responsive zoomable"
                   src="media/output/pyramid_sobel/tobolsk/aligned_(3, 6)_(2, 3)_(1.52).jpg"
                   data-full="media/output/pyramid_sobel/tobolsk/aligned_(3, 6)_(2, 3)_(1.52).jpg"
                   alt="R(3,6) G(2,3) 1.52s">
            </div>
            <p>R(3,6) G(2,3) 1.52s</p>
          </div>
        </div>
      </section>
      <hr>

      <!-- Auto Cropping -->
        <section id="auto-cropping">
          <h3>Post-Processing: Automatic Cropping</h3>
          <p>Once the alignment is done, the colored images often look real enough. However, they typically contain artifacts that reduce their visual quality compared to modern digital photos. Therefore, we here we explore some post-processing techniques to enhance their visual quality.</p>
          <p>The most obvious artifact is the borders, which are typically single color and contain (almost) no information. To address this, here we can remove them through automatic cropping.</p>
          <p>One method is to analyze the edges of the image. The borders are usually nearly uniform in color, with very few edges along the perpendicular direction. Therefore, with the assumption that "the borders are approximately parallel to the image edges", we can detect them through "finding the location where edge gradient changed dramatically" and crop them out. To improve robustness, we analyze the cumulative distribution of the absolute Sobel gradient values. To achieve this, we can operate on the sobel gradient map with respect to $x$ and $y$ directions separately, details are as follows:</p>
          <ol>
            <li>Convert the image to grayscale to keep only the intensity information</li>
            <li>Compute the Sobel gradient maps with respect to the $x$ and $y$ directions, and take their absolute values</li>
            <li>For each gradient map, project the values by summing along the perpendicular axis (e.g., sum columns for the $x$-gradient)</li>
            <li>Compute the cumulative percentage of these projected values, and crop at the positions where the percentage first exceeds a predefined threshold (e.g., $2\%$)</li>
            <li>To avoid over-cropping the edges where the gradients are not significant (e.g, some clean background), the maximum cropping ratio should be limited by a threshold as well (e.g, $7\%$)</li>
          </ol>
          <h4 style="margin-top:10px;">Failure Analysis & Improvement (k-part strategy)</h4>
          <p>The initial implementation of automatic cropping works well for most images; however, some images contain nose on the borders (e.g., writings that records meta information, rugged edges due to poor preservation), which can be recognized as edges as well. Since the initial cropping method is based on detecting the dramatic change of cumulative edge density, those writings on the border cause the density curve to rise quickly and trigger the threshold too early. However, simply increasing the threshold will cause the cropping to degenerate to "simple crop with a fixed ratio", since most other borders do not contain such writings, and the maximum cropping will be simply controlled by the maximum cropping ratio.</p>
          <p>To address this, we can instead adopt a multi-region strategy: instead of applying the cropping detection to the entire image, we divide the image into k vertical (or horizontal) partitions, and perform the same border detection independently on each partition. This is based on the observation that the writings typically appear on a specific region of one border, and the rest of the borders are clean. Therefore, the assumption is that at least one partition will not be heavily influenced by writings or artifacts, and thus can provide a more reasonable cropping boundary. Finally, we take the most aggressive (largest) cropping among the partitions as the final result.</p>
          <h4 style="margin-top:10px;">Results of Automatic Cropping (with & without k-part trick)</h4>
          <p>Some selected results of automatic are demonstrated as follows. One can see that the initial implementation without the k-part trick can already crop the borders appropriately. However, for some cases, the borders cannot beeing cropped cleanly without over-cropping due to noise within borders. For instance, the example one is the case where its right black border contains writings of "5030" at upper part, and the second example shows rugged edges at right and red shades at bottom. After applying aforementioned k-part trick with $k=4$, one can find that the cropped images are cleaner.</p>
          <div class="bw-grid">
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/church/aligned_(-4, 58)_(4, 25)_(16.82).jpg"
                     data-full="media/output/pyramid_sobel/church/aligned_(-4, 58)_(4, 25)_(16.82).jpg"
                     alt="Before cropping (raw aligned)">
              </div>
              <p class="bw-cap">Before cropping</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/church/aligned_cropped(k=1).png"
                     data-full="media/output/pyramid_sobel/church/aligned_cropped(k=1).png"
                     alt="Cropped without k-part trick">
              </div>
              <p class="bw-cap">Cropped w/o k-part trick</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/church/aligned_cropped.jpg"
                     data-full="media/output/pyramid_sobel/church/aligned_cropped.jpg"
                     alt="Cropped with k-part trick (k=4)">
              </div>
              <p class="bw-cap">Cropped w/ k-part trick (k=4)</p>
            </div>
          </div>

                    <div class="bw-grid">
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/lastochikino/aligned_(-8, 76)_(-2, -3)_(17.47).jpg"
                     data-full="media/output/pyramid_sobel/lastochikino/aligned_(-8, 76)_(-2, -3)_(17.47).jpg"
                     alt="Before cropping (raw aligned)">
              </div>
              <p class="bw-cap">Before cropping</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/lastochikino/aligned_cropped(k=1).png"
                     data-full="media/output/pyramid_sobel/lastochikino/aligned_cropped(k=1).png"
                     alt="Cropped without k-part trick">
              </div>
              <p class="bw-cap">Cropped w/o k-part trick</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/lastochikino/aligned_cropped.jpg"
                     data-full="media/output/pyramid_sobel/lastochikino/aligned_cropped.jpg"
                     alt="Cropped with k-part trick (k=4)">
              </div>
              <p class="bw-cap">Cropped w/ k-part trick (k=4)</p>
            </div>
          </div>
        </section>
        <h4 style="margin-top:10px;">Trade-Off Analysis</h4>
        <p>However, there is still a trade-off. After observing the aligned images, one would ask: "How do we define the so-called borders?" Are they just single colored straight bands? Or we should also consider the edge regions that are mainly shaded by single colors while also containing semantic information? Cutting the latter out or adjusting them back to their natural color was a trade-off that took me a long time to consider. The following $melons$ is a good example, it contains a large region of multiple single-colored shaded bands on the top, which apparently is the top of the building behind the fruit farmer; however, if we consider them are borders, they are all cut off and we lose all the information! The ideal way to handle them should be mapping them back to natural colors; however, this seems too hard to automate. Therefore, in my final implementation, I tuned the thresholds to make the cropping not too aggressive, then, we can still keep some of those regions to keep the information inside, even though they may look ugly.</p>
                  <div class="bw-grid">
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/melons/aligned_(12, 177)_(10, 80)_(17.73).jpg"
                     data-full="media/output/pyramid_sobel/melons/aligned_(12, 177)_(10, 80)_(17.73).jpg"
                     alt="Before cropping (raw aligned)">
              </div>
              <p class="bw-cap">Before cropping</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/melons/aligned_cropped.jpg"
                     data-full="media/output/pyramid_sobel/melons/aligned_cropped.jpg"
                     alt="Cropped with k-part trick (k=4), keeping some edge regions">
              </div>
              <p class="bw-cap">Cropped w/ k-part trick (k=4), keeping some edge regions (top)</p>
            </div>
          </div>
        <hr>
        
        <!-- Auto White Balancing -->
        <section id="auto-wb">
          <h3>Post-Processing: Automatic White Balancing</h3>
          <p>Limited by the technology of Prokudin-Gorskii's time, the colors of aligned images are typically not natural enough. To enhance the visual quality, here we manipulate the pixel intensities and distributions as post-processing.</p>
          <p>From observations, it is common that the aligned colored photos exhibit <a href="https://en.wikipedia.org/wiki/Colour_cast">color casts</a>, i.e., the overall color shifts toward one color. Therefore, to make the colors more natural, here we correct the color through automatic <a href="https://en.wikipedia.org/wiki/Color_balance">white balancing</a>, where we identify and adjust the white points to be displayed as pure white. Like other color constancy algorithms, white balancing consists of two steps:</p>
          <ol>
            <li>Determining the illuminant under which an image was captured</li>
            <li>Adjusting the colors to counteract the illuminant and simulate a neutral illuminant</li>
          </ol>
          <p>There are <a href="https://python.plainenglish.io/introduction-to-image-processing-with-python-bb39c83366a4">various algorithms</a> for automatic white balancing, where the most straightforward yet effective ones are the <a href="https://medium.com/@weichenpai/gray-world-assumption-in-computer-vision-0a6612c1420a">Gray World Assumption</a> and White Patch Assumption. The former assumes that the average reflectance in a scene is achromatic (gray), while the latter assumes that there exists at least one pixel in perfect white (equal RGB values), both of which serves as reference for step 1. Once we have the illuminant information from step 1, correcting the colors (step 2) is simply a linear transformation that scales the RGB channels accordingly.</p>
          <p>The linear transformation for adjusting RGB channels can be expressed as a $3 \times 3$ matrix multiplication on the raw RGB values. With the assumption that the image was taken under a single illuminant, and that Prokudin-Gorskii's RGB filters were ideal such that the RGB channels are uncoupled (similar to the <a href="https://en.wikipedia.org/wiki/Von_Kries_coefficient_law">Von Kries Coefficient Law</a> that human cones are adjusted separately), then the transformation matrix can be simplified to a diagonal form with per-channel scaling factors:</p>
          <div class="math-block">$$
          \begin{bmatrix} R \\[2pt] G \\[2pt] B \end{bmatrix} =
          \begin{bmatrix}
          \alpha & 0 & 0\\
          0 & \beta & 0\\
          0 & 0 & \gamma
          \end{bmatrix}
          \begin{bmatrix} R' \\[2pt] G' \\[2pt] B' \end{bmatrix}
          $$</div>
          <p>where $[R', G', B']^{\top}$ are the original RGB values before white balancing, $[R, G, B]^{\top}$ are the adjusted ones. The scaling factors $\alpha, \beta, \gamma$ are determined based on the illuminant estimated from step 1.</p>
          <p>But one may ask, which algorithm should we implement for this project? Here we can analyze the photos we have, together with the properties of each algorithm. One common pattern we can observe in Prokudin-Gorskii's collection is that most of them exhibit darkened edges, shadows, or aging artifacts, which bias the global average toward darker or desaturated tones. As a result, applying the Gray World Assumption tends to introduce undesirable color shifts.</p>
          <p>On the other hand, by analyzing the subject matter of Prokudin-Gorskii's photographs, we can find that they are mainly landscapes, architecture and humanities, which typically contain some white objects. Therefore, trading off the two algorithms, implementing the White Patch Assumption is a better choice for this project.</p>
          <p>Recall that the White Patch Assumption calibrates the photo based on the brightest pixels in each channel, say, $R_{\max}, G_{\max}, B_{\max}$, suppose our intensity range is defined as $[I_{\min}, I_{\max}]$, then the scaling factors can be naively determined as:</p>
          <div class="math-block">$$
          \alpha=\frac{I_{\max}}{R_{\max}},\quad \beta=\frac{I_{\max}}{G_{\max}},\quad \gamma=\frac{I_{\max}}{B_{\max}}
          $$</div>
          <p>However, simply applying the White Patch Assumption on the single brightest pixel of each channel may be sensitive to noise and extreme cases, e.g., some noise pixels are extremely bright, or some images contain saturated regions that are very bright in only one channel, both of which can potentially harm the performance. To address this and enhance the robustness, my implementation involves two strategies:</p>
          <ol>
            <li>Only consider pixels whose channel values are mutually similar (within a threshold $\delta$), which are more likely to be near-white/neutral rather than single-colored highlights.</li>
            <li>Instead of using the single maximum, use the average over the $\texttt{top_p}\%$ brightest pixels (measured in grayscale luminance).</li>
          </ol>
          <p>With the constraints above, we now only consider the pixel $i$ that are both bright (i.e., high grayscale intensity $L_i$) and approximately neutral in chromaticity, which are collected into the set:</p>
          <div class="math-block">$$
          \Omega = \left\{\, i \;\middle|\; L_i \ge Q_p(L),\;
          \max\!\left(
          \frac{|R_i-G_i|}{\max(R_i,G_i)},\;
          \frac{|G_i-B_i|}{\max(G_i,B_i)},\;
          \frac{|B_i-R_i|}{\max(B_i,R_i)}
          \right) \le \delta \right\}
          $$</div>
          <p>where $Q_p(L)$ is the threshold corresponding to the $\texttt{top_p}\%$ brightest pixels in the grayscale distribution (set to $\texttt{top_p}=2$ in my implementation, i.e., keep the top $2\%$ brightest pixels), and $\delta$ is the threshold that controls the chromaticity similarity (implemented to be $\delta=0.05$ in my case). Then, the $R_{\max}, G_{\max}, B_{\max}$ are computed as the averages of the corresponding channels over the set $\Omega$:</p>
          <div class="math-block">$$
          R_{\max}=\frac{1}{|\Omega|}\!\sum_{i\in\Omega} R_i,\quad
          G_{\max}=\frac{1}{|\Omega|}\!\sum_{i\in\Omega} G_i,\quad
          B_{\max}=\frac{1}{|\Omega|}\!\sum_{i\in\Omega} B_i
          $$</div>
          <p>After scaling, pixels whose intensities exceed the range of $[I_{\min}, I_{\max}]$ are clipped to the nearest boundary.</p>

          <!-- White Balance before/after -->
          <h4 style="margin-top:10px;">Results of Automatic White Balancing</h4>
          <p>Some selected results are shown below. In my implementation, I aimed for a non-aggressive white balancing adjustment, so the $\texttt{top_p}$ and $\delta$ were set to $2$ and $0.05$, respectively. For example, the roof of the fruit farmer and the hat of the Emir were balanced to white appropriately, demonstrating that the method can effectively correct global color casts while preserving a natural appearance.</p>
          <div class="bw-grid">
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/melons/aligned_cropped.jpg"
                     data-full="media/output/pyramid_sobel/melons/aligned_cropped.jpg"
                     alt="Before white balance">
              </div>
              <p class="bw-cap">After Cropping, before WB</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/melons/aligned_cropped_wb.jpg"
                     data-full="media/output/pyramid_sobel/melons/aligned_cropped_wb.jpg"
                     alt="After white balance">
              </div>
              <p class="bw-cap">After WB</p>
            </div>
          </div>

          <div class="bw-grid">
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/emir/aligned_cropped.jpg"
                     data-full="media/output/pyramid_sobel/emir/aligned_cropped.jpg"
                     alt="Before white balance">
              </div>
              <p class="bw-cap">After Cropping, before WB</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/emir/aligned_cropped_wb.jpg"
                     data-full="media/output/pyramid_sobel/emir/aligned_cropped_wb.jpg"
                     alt="After white balance">
              </div>
              <p class="bw-cap">After WB</p>
            </div>
          </div>
        </section>
        <hr>

        <section id="auto-contrast">
          <h3>Post-Processing: Automatic Contrasting</h3>
          <p>Another common issue in Prokudin-Gorskii's collection is low-contrast, which means, the <a href="https://en.wikipedia.org/wiki/Image_histogram">image histograms</a> are not spanning the full intensity range, yielding details in some regions less distinguishable. Therefore, subsequent contrast adjustment is necessary to make weak signals more visible, revealing details that were already present in the data but previously hidden in low-contrast regions. Here we enhance the contrast by manipulating the image histograms. </p>
          <p>The simplest way is to map the histogram linearly to span the full intensity range, namely <a href="https://samirkhanal35.medium.com/contrast-stretching-f25e7c4e8e33">contrast stretching</a>. Say, for the grayscale space with the intensity range of $[I_{\min}, I_{\max}]$ and total number of gray levels $LI$ (e.g., for uint8, $LI=256$ and range is $[0,255]$), if we want to stretch an image, whose maximum and minimum intensities are $r_{\max}$ and $r_{\min}$, then the function mapping the original intensity $r$ to the new one $s$ (assuming $I_{\min}=0$ and $I_{\max}=LI-1$) is defined as:</p>
          <div class="math-block">$$
          s = \frac{r-r_{\min}}{r_{\max}-r_{\min}}(I_{\max}-I_{\min}) + I_{\min}
          = \frac{r-r_{\min}}{r_{\max}-r_{\min}}(LI-1)
          $$</div>
          <p>However, one can observe that this method is sensitive to noise and outliers, since it relies on $r_{\min}$ and $r_{\max}$ heavily, hence, any noise/outliers that deviate off the main distribution will affect the result significantly. Moreover, such a linear mapping does not change the shape of histogram, therefore, cannot effectively enhance regions where pixel intensities are densely clustered (i.e., locally low-contrast regions due to over/under exposure).</p>
          <p>A more robust way is <a href="https://en.wikipedia.org/wiki/Histogram_equalization">histogram equalization (HE)</a>, which manipulates the cumulative distribution function (CDF) of the pixel intensities. Starting from the image histogram, one can normalize it to get the probability distribution function (PDF) and then get the CDF through cumulative summation, which is basically the empirical CDF (ECDF) of the pixel intensities, defined as:</p>
          <div class="math-block">$$
          \hat{F}_n(r) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(R_i \le r)
          $$</div>
          <p>where $R_i$ is the intensity of pixel $i \in \{1,2,...,n\}$ that belongs to one of $n$ pixels of the image, and $\mathbf{1}(\cdot)$ is the indicator function.</p>
          <p>Then, we can use ECDF to construct the mapping function from original intensity $r$ to new intensity $s$ that satisfies the requirement that $T: [I_{\min}, I_{\max}] \to [I_{\min}, I_{\max}]$, which is simply a scaled version of ECDF:</p>
          <div class="math-block">$$
          s = T(r) = \hat{F}_n(r)(I_{\max}-I_{\min}) + I_{\min} = \hat{F}_n(r)(LI-1)
          $$</div>
          <p>From the definition, one can observe that this method is less sensitive to outliers, and can redistribute the pixel intensities more evenly, hence, able to address the aforementioned issues of contrast stretching.</p>
          <p>However, it still has limitations in our case:</p>
          <ol>
            <li>Both of above methods operate globally, however, the photos we have typically contain noise and defects in some local regions (e.g., uneven aging artifacts and unusual monochrome shades around the edges). Such local defects and noise will be incorporated into the new histogram, propagated to the whole image, and affect the overall contrast enhancement.</li>
            <li>Since HE redistributes pixel intensities to approximate a uniform distribution, originally dense regions may be excessively spread out, while sparse regions are filled with additional pixels, sometimes introducing artificial contrast patterns or visual artifacts.</li>
          </ol>
          <p>With such issues in mind, the final implementation adopts the <a href="https://en.wikipedia.org/wiki/Adaptive_histogram_equalization#CLAHE">Contrast Limited Adaptive Histogram Equalization (CLAHE)</a>, which:</p> 
          <ol>
            <li>Operates HE on small tile regions locally and combines the results via bilinear interpolation, addressing the first issue.</li>
            <li>To alleviate the second issue, HE on each tile is operated on a clipped and re-distributed histogram (through a parameter $\texttt{clip_limit}$), yielding better local contrast enhancement without over-amplifying noise.</li>
          </ol>
          <p>Luckily, skimage provides a well-implemented function for CLAHE, so I used <a href="https://scikit-image.org/docs/0.25.x/api/skimage.exposure.html#skimage.exposure.equalize_adapthist"> $\texttt{skimage.exposure.equalize_adapthist}$</a> for my final implementation.</p>

          <h4 style="margin-top:10px;">Results of Automatic Contrasting</h4>
          <p>Some selected results are shown below. Again, to avoid over-enhancement, I set the parameter $\texttt{clip_limit}$ to be a small value ($0.004$). One can see that after applying CLAHE, the details in some low-contrast regions are revealed, e.g., the texture of the fruit, and the details of Emir's beautiful cloth. </p>
          <!-- Contrast (CLAHE) -->
          <div class="bw-grid">
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/melons/aligned_cropped_wb.jpg"
                     data-full="media/output/pyramid_sobel/melons/aligned_cropped.jpg"
                     alt="Before contrast (after WB)">
              </div>
              <p class="bw-cap">After Cropping + WB, before contrast</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/melons/aligned_cropped_wb_contrast.jpg"
                     data-full="media/output/pyramid_sobel/melons/aligned_cropped_wb_contrast.jpg"
                     alt="After CLAHE">
              </div>
              <p class="bw-cap">After CLAHE</p>
            </div>
          </div>

          <div class="bw-grid">
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/emir/aligned_cropped_wb.jpg"
                     data-full="media/output/pyramid_sobel/emir/aligned_cropped.jpg"
                     alt="Before contrast (after WB)">
              </div>
              <p class="bw-cap">After Cropping + WB, before contrast</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/pyramid_sobel/emir/aligned_cropped_wb_contrast.jpg"
                     data-full="media/output/pyramid_sobel/emir/aligned_cropped_wb_contrast.jpg"
                     alt="After CLAHE">
              </div>
              <p class="bw-cap">After CLAHE</p>
            </div>
          </div>
        </section>
        <hr>

        <section id="better-colormap">
          <h3>Post-Processing: Better Color Mapping</h3>
          <p>TBD</p>
          <!-- Color transfer (Âç†‰ΩçÁ§∫‰æã) -->
          <div class="bw-grid">
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/large_pyramid_sobel/emir_cropped_wb_contrast.jpg"
                     data-full="media/output/large_pyramid_sobel/emir_cropped_wb_contrast.jpg"
                     alt="Before color transfer">
              </div>
              <p class="bw-cap">Before color transfer</p>
            </div>
            <div class="bw-card">
              <div class="bw-frame">
                <img class="responsive zoomable"
                     src="media/output/large_pyramid_sobel/emir_color_transfer.jpg"
                     data-full="media/output/large_pyramid_sobel/emir_color_transfer.jpg"
                     alt="After color transfer">
              </div>
              <p class="bw-cap">After color transfer</p>
            </div>
          </div>
        </section>
      </section>

      <!-- footer -->
      <footer>
        <p>¬© 2025 ZHOU Guanren |
          <a href="mailto:guanren_zhou@berkeley.edu">üìß Email</a> |
          <a href="https://github.com/Nutlettt" target="_blank" rel="noopener noreferrer">
            <img src="../media/github.png" alt="GitHub" style="width:16px;height:16px;vertical-align:middle;margin-right:4px;">
            GitHub
          </a>
        </p>
      </footer>
    </div>
  </main>

  <!-- Lightbox (click-to-view original) -->
  <div class="lightbox" id="pg-lightbox">
    <div class="lb-backdrop"></div>
    <div class="lb-inner">
      <img id="lb-img" alt="">
      <div class="lb-caption" id="lb-cap"></div>
      <button class="lb-close" id="lb-close" aria-label="Close">&times;</button>
    </div>
  </div>
  <script>
  (function(){
    /* ---- Sidebar toggle (ÂéüÊ†∑) ---- */
    const sidebar = document.getElementById('sidebar');
    const overlay = document.getElementById('sidebar-overlay');
    const sidebarBtn = document.getElementById('sidebar-toggle');
    function closeSidebar(){ sidebar.classList.add('collapsed'); overlay.classList.remove('open'); }
    function toggleSidebar(){ const collapsed = sidebar.classList.toggle('collapsed'); overlay.classList.toggle('open', !collapsed); }
    sidebarBtn && sidebarBtn.addEventListener('click', toggleSidebar);
    overlay && overlay.addEventListener('click', closeSidebar);
    document.addEventListener('keydown', e=>{ if(e.key==='Escape') closeSidebar(); });

    /* ---- Theme toggle (ÂÖºÂÆπÂ§öÁßçÂÜôÊ≥ï) ---- */
    const themeBtn = document.getElementById('theme-toggle');
    const root = document.documentElement;
    function applyTheme(t){
      // data-attr
      root.setAttribute('data-theme', t);
      document.body.setAttribute('data-theme', t);
      // Â∏∏ËßÅÁ±ªÂêçÂÖ®ÈÉ®ÂêåÊ≠•‰∏ÄÈÅçÔºåÈÄÇÈÖç‰Ω†ÊóßÁöÑ CSS
      const darkOn = t === 'dark';
      const classes = ['dark','dark-theme','theme-dark','mode-dark'];
      classes.forEach(c=>{
        root.classList.toggle(c, darkOn);
        document.body.classList.toggle(c, darkOn);
      });
      // ‰πüÁªô light ÁâàÊú¨ÔºàÈÉ®ÂàÜÊ†∑ÂºèÁî® .lightÔºâ
      const lightClasses = ['light','light-theme','theme-light','mode-light'];
      lightClasses.forEach(c=>{
        root.classList.toggle(c, !darkOn);
        document.body.classList.toggle(c, !darkOn);
      });
      localStorage.setItem('theme', t);
      if (themeBtn) themeBtn.title = 'Switch to ' + (darkOn ? 'light' : 'dark') + ' theme';
    }
    const stored = localStorage.getItem('theme');
    const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
    applyTheme(stored || (prefersDark ? 'dark' : 'light'));

    themeBtn && themeBtn.addEventListener('click', ()=>{
      const next = (root.getAttribute('data-theme') === 'dark') ? 'light' : 'dark';
      applyTheme(next);
    });

    // Ë∑üÈöèÁ≥ªÁªüÂèòÂåñÔºàÂèØÈÄâÔºâ
    window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', e=>{
      if(!localStorage.getItem('theme')) applyTheme(e.matches ? 'dark' : 'light');
    });

    /* ---- Lightbox (ÂéüÊ†∑) ---- */
    const lb = document.getElementById('pg-lightbox');
    if(lb){
      const imgEl = document.getElementById('lb-img');
      const capEl = document.getElementById('lb-cap');
      const closeBtn = document.getElementById('lb-close');
      function open(src, cap){ imgEl.src = src; capEl.textContent = cap || ''; lb.classList.add('open'); }
      function close(){ lb.classList.remove('open'); imgEl.src=''; }
      document.querySelectorAll('img.zoomable').forEach(im=>{
        im.addEventListener('click', ()=> open(im.getAttribute('data-full')||im.src, im.alt));
      });
      lb.addEventListener('click', e=>{ if(e.target===lb || e.target.classList.contains('lb-backdrop')) close(); });
      closeBtn && closeBtn.addEventListener('click', close);
      document.addEventListener('keydown', e=>{ if(e.key==='Escape' && lb.classList.contains('open')) close(); });
    }
  })();
  </script>
</body>
</html>
